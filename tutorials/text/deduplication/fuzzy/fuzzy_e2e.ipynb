{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe55f10a-8210-4746-9ddd-f7ad60a5cc52",
   "metadata": {},
   "source": [
    "# End-to-end Fuzzy Deduplication\n",
    "\n",
    "GPU accelerated implementation of a MinHash-LSH based fuzzy deduplication. For more information about semantic deduplication in NeMo Curator, refer to the [Deduplication](https://docs.nvidia.com/nemo/curator/latest/curate-text/process-data/deduplication/index.html) section of the documentation page.\n",
    "\n",
    "The tutorial here shows how to run Fuzzy Duplication on text data by executing a 2 end to end workflows which does the following:\n",
    "\n",
    "1. Read original dataset\n",
    "2. Compute MinHashes signatures of these documents\n",
    "3. Perform LSH - Group Minhashes into bands/buckets and shuffle these bands/buckets so that documents in the same bucket are in the same batch/file.\n",
    "4. Convert the LSH outputs (bucket_id -> doc_id mapping) into a edgelist in preperation for connected components. \n",
    "5. Compute connected components across all potential duplicates found via LSH.\n",
    "6. Generate list of duplicate documents by randomly selecting 1 document to keep from each group/component and dropping the rest.\n",
    "7. Remove duplicates based on the generated duplicate list.\n",
    "\n",
    "We also allow users to also run these steps independently, which will be covered in the step by step tutorial in the same directory as this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4a272b-4ea6-4b03-9f57-be4cd16812cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Silence Curator logs via Loguru\n",
    "os.environ[\"LOGURU_LEVEL\"] = \"ERROR\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_path = os.path.abspath(\"./input\")  # Path to input dataset\n",
    "fuzzy_output_dir = os.path.abspath(\"./output/e2e\")  # Path to store all fuzzy outputs including cache & deduped dataset\n",
    "cache_path = os.path.join(\n",
    "    fuzzy_output_dir, \"cache\"\n",
    ")  # Path to store fuzzy deduplication intermediates (minhash, lsh etc.)\n",
    "deduplicated_output_path = os.path.join(fuzzy_output_dir, \"fuzzy_deduped_dataset\")\n",
    "\n",
    "input_filetype = (\n",
    "    \"parquet\"  # this can be either of jsonl or parquet (you'll need to change how input data is generated)\n",
    ")\n",
    "output_filetype = \"parquet\"  # this can be either of jsonl or parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f399d1-2486-4b8d-bd40-d3ddbd77dbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 212 files\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.utils.file_utils import get_all_file_paths_under\n",
    "\n",
    "if len(get_all_file_paths_under(input_path)) == 0:\n",
    "    import os\n",
    "    import uuid\n",
    "\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    input_df = load_dataset(\"roneneldan/TinyStories\", split=\"train\").to_pandas()\n",
    "    num_rows_per_file = 10_000\n",
    "\n",
    "    os.makedirs(input_path, exist_ok=True)\n",
    "\n",
    "    for i, start_idx in enumerate(range(0, len(input_df), num_rows_per_file)):\n",
    "        end_idx = min(len(input_df), start_idx + num_rows_per_file)\n",
    "        subset_df = input_df.iloc[start_idx:end_idx].copy()\n",
    "        subset_df[\"id\"] = [str(uuid.uuid4()) for _ in range(len(subset_df))]\n",
    "        subset_df.to_parquet(os.path.join(input_path, f\"part_{i}.parquet\"), index=False)\n",
    "\n",
    "    print(f\"Created {len(os.listdir(input_path))} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabcb72d-6d22-4324-a51c-73241f0d9d19",
   "metadata": {},
   "source": [
    "## Running as a Single Stage (End-to-End)\n",
    "\n",
    "See the [API Reference](https://docs.nvidia.com/nemo/curator/latest/apidocs/stages/stages.deduplication.fuzzy.workflow.html#api) for more information about the `FuzzyDeduplicationWorkflow` class.\n",
    "\n",
    "### General Notes\n",
    "#### ID Generation\n",
    "1. The Fuzzy Deduplication Workflow doesn't utilize any existing IDs in the input dataset and instead generates IDs on the fly using an ID Generator actor.\n",
    "2. The ID Generator gives each row a unique increasing integer ID, based on the order files are read.\n",
    "3. This avoids expensive ID->Integer encoding for the underlying connected components algorithm which only supports integer IDs.\n",
    "4. When we find duplicates, we save these integer IDs in sorted files with multiple row groups.\n",
    "5. We also save a `fuzzy_id_generator.json` which maintains a mapping of input file partitions to ID ranges for that batch.\n",
    "6. During removal, reading the same file groups will give the same integer IDs, using the min/max ID values, we can find all corresponding duplicates in that range making the process faster.\n",
    "\n",
    "#### Performance Considerations\n",
    "1. LSH - Configuring bands_per_iteration controls how many bands to process simultanesouly in a single shuffle. Higher values can lead to faster performance but might increase memory pressure.\n",
    "2. A low `input_blocksize` may not saturate the GPUs enough while a high `input_blocksize` can lead to OOM errors during MinHash and excessive object store usage during removal. It's recommend to keep it at 1-1.5GiB and reduce if running into OOMs during MinHash.\n",
    "3. The removal step can be memory intensive and it's recommend to set a higher fraction of object store memory for removal (if the machine has enough RAM). The `RayDataExecutor` showed better results during duplicate removal.\n",
    "4. The removal workflow is CPU only and can be run  on machines that don't have GPUs\n",
    "\n",
    "#### Hyperparameter Considerations\n",
    "1. The current defaults for Fuzzy deduplication (260 hashes, 13 hashes per band) approximate finding documents with a jaccard similarity of 0.8. For more information on selecting the number of bands/hashes it's recommended to analyze the S curve and tolerable threshold for false postives (and negatives). More information about LSH can be found in section `3.4.2` [here](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf).\n",
    "2. The `char_ngrams` values of 24 is set to approximate roughly ngrams that correspond to ~5 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf7a7a3-4659-45b3-be86-5803efb54f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.stages.deduplication.fuzzy import FuzzyDeduplicationWorkflow\n",
    "from nemo_curator.stages.deduplication.id_generator import CURATOR_DEDUP_ID_STR\n",
    "from nemo_curator.stages.text.deduplication import TextDuplicatesRemovalWorkflow\n",
    "\n",
    "identification_workflow = FuzzyDeduplicationWorkflow(\n",
    "    cache_path=cache_path,\n",
    "    output_path=fuzzy_output_dir,\n",
    "    input_path=input_path,\n",
    "    input_filetype=input_filetype,\n",
    "    input_blocksize=\"1GiB\",\n",
    "    text_field=\"text\",\n",
    "    seed=42,\n",
    "    char_ngrams=24,\n",
    "    minhashes_per_band=13,\n",
    "    bands_per_iteration=10,\n",
    ")\n",
    "\n",
    "removal_workflow = TextDuplicatesRemovalWorkflow(\n",
    "    input_path=input_path,\n",
    "    ids_to_remove_path=os.path.join(fuzzy_output_dir, \"FuzzyDuplicateIds\"),\n",
    "    output_path=deduplicated_output_path,\n",
    "    input_filetype=input_filetype,\n",
    "    input_blocksize=\"1GiB\",\n",
    "    ids_to_remove_duplicate_id_field=CURATOR_DEDUP_ID_STR,\n",
    "    id_generator_path=os.path.join(fuzzy_output_dir, \"fuzzy_id_generator.json\"),\n",
    "    output_filetype=\"parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7329ff2-3a36-42ad-833a-9589653d0e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 14:07:41,489\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:07:41,491\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "[2025-11-18 14:07:46,761 W 2783477 2783477] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-11-18 14:07:47,763 W 2783477 2783477] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-11-18 14:07:48,764 W 2783477 2783477] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-18 14:07:43,899\tINFO usage_lib.py:447 -- Usage stats collection is disabled.\n",
      "2025-11-18 14:07:43,899\tINFO scripts.py:914 -- \u001b[37mLocal node IP\u001b[39m: \u001b[1m127.0.1.1\u001b[22m\n",
      "2025-11-18 14:07:48,884\tSUCC scripts.py:950 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-11-18 14:07:48,884\tSUCC scripts.py:951 -- \u001b[32mRay runtime started.\u001b[39m\n",
      "2025-11-18 14:07:48,884\tSUCC scripts.py:952 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-11-18 14:07:48,884\tINFO scripts.py:954 -- \u001b[36mNext steps\u001b[39m\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:957 -- To add another node to this Ray cluster, run\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:960 -- \u001b[1m  ray start --address='127.0.1.1:6379'\u001b[22m\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:969 -- To connect to this Ray cluster:\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:971 -- \u001b[35mimport\u001b[39m\u001b[26m ray\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:972 -- ray\u001b[35m.\u001b[39m\u001b[26minit(_node_ip_address\u001b[35m=\u001b[39m\u001b[26m\u001b[33m'127.0.1.1'\u001b[39m\u001b[26m)\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:984 -- To submit a Ray job using the Ray Jobs CLI:\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:985 -- \u001b[1m  RAY_API_SERVER_ADDRESS='http://127.0.0.1:8265' ray job submit --working-dir . -- python my_script.py\u001b[22m\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:994 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:998 -- for more information on submitting Ray jobs to the Ray cluster.\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:1003 -- To terminate the Ray runtime, run\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:1004 -- \u001b[1m  ray stop\u001b[22m\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:1007 -- To view the status of the cluster, use\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:1008 --   \u001b[1mray status\u001b[22m\u001b[26m\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:1012 -- To monitor and debug Ray, view the dashboard at \n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:1013 --   \u001b[1m127.0.0.1:8265\u001b[22m\u001b[26m\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:1020 -- \u001b[4mIf connection to the dashboard fails, check your firewall settings and network configuration.\u001b[24m\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:1121 -- \u001b[36m\u001b[1m--block\u001b[22m\u001b[39m\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:1122 -- This command will now block forever until terminated by a signal.\n",
      "2025-11-18 14:07:48,885\tINFO scripts.py:1125 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-18 14:07:49,765 I 2783477 2783477] global_state_accessor.cc:487: This node has an IP address of 127.0.1.1, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.\n",
      "2025-11-18 14:07:49,767\tINFO worker.py:2013 -- Connected to Ray cluster.\n",
      "/raid/adattagupta/NeMo-Curator/.venv/lib/python3.12/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "2025-11-18 14:07:54,429\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:07:54,430\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "2025-11-18 14:07:54,436\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-11-18 14:08:13,403\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:08:13,405\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "2025-11-18 14:08:13,411\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-11-18 14:08:37,446\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:08:37,448\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "2025-11-18 14:08:37,454\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-11-18 14:09:24,089\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:09:24,090\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "2025-11-18 14:09:24,096\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-11-18 14:09:24,121\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:09:24,122\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "2025-11-18 14:09:24,122\tINFO worker.py:1851 -- Calling ray.init() again after it has already been called.\n",
      "2025-11-18 14:09:24,606\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:09:24,607\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "2025-11-18 14:09:24,612\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-11-18 14:09:31,621\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_5_0\n",
      "2025-11-18 14:09:31,634\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_5_0. Full logs are in /tmp/ray/session_2025-11-18_14-07-44_055756_2784442/logs/ray-data\n",
      "2025-11-18 14:09:31,635\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_5_0: InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(FilePartitioningStageTask)] -> TaskPoolMapOperator[StreamingRepartition] -> ActorPoolMapOperator[MapBatches(ParquetReaderStageActor)] -> TaskPoolMapOperator[MapBatches(TextDuplicatesRemovalStageTask)->MapBatches(ParquetWriterTask)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c7a2d092bb497b9a45da5f0065a7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-18 14:09:31,674 E 2783477 2783477] core_worker.cc:2153: Actor with class name: 'MapWorker(MapBatches(ParquetReaderStageActor))' and ID: 'fc35e5fe28e1db1b3d820d5e06000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9e538df4a5469e87f4133771fe0dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(FilePartitioningStageTask) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeebcd4823854b6fb54f1d081d2e83b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- StreamingRepartition 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57266c07ef34f39ac6d39b6736b820e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(ParquetReaderStageActor) 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ace8170e4a4801a52cf5a9f39d54be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(TextDuplicatesRemovalStageTask)->MapBatches(ParquetWriterTask) 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 14:09:31,699\tWARNING resource_manager.py:134 -- ⚠️  Ray's object store is configured to use only 10.4% of available memory (186.3GiB out of 1786.3GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2025-11-18 14:09:57,466\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_5_0 execution finished in 25.83 seconds\n",
      "2025-11-18 14:09:57,538\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:09:57,540\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "2025-11-18 14:09:57,546\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.backends.experimental.ray_data import RayDataExecutor\n",
    "from nemo_curator.core.client import RayClient\n",
    "\n",
    "client = RayClient(num_cpus=64, num_gpus=2)  # change as needed\n",
    "client.start()\n",
    "\n",
    "_ = identification_workflow.run()\n",
    "_ = removal_workflow.run(executor=RayDataExecutor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c983f3-f562-45e6-bd09-5660cb179aa3",
   "metadata": {},
   "source": [
    "### Looking at Intermediate Results and Output\n",
    "\n",
    "#### MinHash Results\n",
    "1. `_curator_dedup_id` - The IDs assigned to this dataset on the fly during the intial read.\n",
    "2. `_minhash_signature` - MinHash Signature\n",
    "\n",
    "#### LSH Results\n",
    "1. `_bucket_id` - The bucket/band identifier\n",
    "2. `_curator_dedup_id` - List of all document IDs that belong to that bucket\n",
    "\n",
    "#### Buckets To Edges Result\n",
    "1. `_curator_dedup_id_x`, `_curator_dedup_id_y` - Mapping of edges in a Graph where each column are documents that are potential duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d9b66f-0d48-4a90-96d6-570fd218777c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_minhash_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[11644717, 429172, 6014805, 86354, 2387151, 49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[2103321, 653305, 2941429, 5780991, 6977799, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[1891498, 3797631, 2961751, 50078, 21382505, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[1286357, 4060996, 1376561, 3044837, 7369355, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[6272013, 12535265, 819579, 5975720, 25677928,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id                                 _minhash_signature\n",
       "0                  0  [11644717, 429172, 6014805, 86354, 2387151, 49...\n",
       "1                  1  [2103321, 653305, 2941429, 5780991, 6977799, 7...\n",
       "2                  2  [1891498, 3797631, 2961751, 50078, 21382505, 5...\n",
       "3                  3  [1286357, 4060996, 1376561, 3044837, 7369355, ...\n",
       "4                  4  [6272013, 12535265, 819579, 5975720, 25677928,..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_bucket_id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0_000055fd7daae1e46223e8b7e06bf2e0</td>\n",
       "      <td>[1158375, 2079489]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0_00006a5f30f7b2c96588bfc1bfb5321a</td>\n",
       "      <td>[365218, 1933514]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0_00006f316d5bd251bd83702e3f1e017f</td>\n",
       "      <td>[161590, 771961]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b0_0000d90e9e4140a7ac31e6b227a62f62</td>\n",
       "      <td>[8290, 567169]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b0_0000f975e5bcda25838df43b0d37737f</td>\n",
       "      <td>[965853, 1334885]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _bucket_id   _curator_dedup_id\n",
       "0  b0_000055fd7daae1e46223e8b7e06bf2e0  [1158375, 2079489]\n",
       "1  b0_00006a5f30f7b2c96588bfc1bfb5321a   [365218, 1933514]\n",
       "2  b0_00006f316d5bd251bd83702e3f1e017f    [161590, 771961]\n",
       "3  b0_0000d90e9e4140a7ac31e6b227a62f62      [8290, 567169]\n",
       "4  b0_0000f975e5bcda25838df43b0d37737f   [965853, 1334885]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id_x</th>\n",
       "      <th>_curator_dedup_id_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1158375</td>\n",
       "      <td>2079489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>365218</td>\n",
       "      <td>1933514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161590</td>\n",
       "      <td>771961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8290</td>\n",
       "      <td>567169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>965853</td>\n",
       "      <td>1334885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id_x  _curator_dedup_id_y\n",
       "0              1158375              2079489\n",
       "1               365218              1933514\n",
       "2               161590               771961\n",
       "3                 8290               567169\n",
       "4               965853              1334885"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minhash_path = os.path.join(cache_path, \"MinHashStage\")\n",
    "display(pd.read_parquet(os.path.join(minhash_path, os.listdir(minhash_path)[0])).head())\n",
    "\n",
    "lsh_path = os.path.join(cache_path, \"LSHStage\")\n",
    "display(pd.read_parquet(os.path.join(lsh_path, os.listdir(lsh_path)[0])).head())\n",
    "\n",
    "b2e_path = os.path.join(cache_path, \"BucketsToEdgesStage\")\n",
    "display(pd.read_parquet(os.path.join(b2e_path, os.listdir(b2e_path)[0])).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650db74e-b764-44fa-966a-e4f0ddcb7182",
   "metadata": {},
   "source": [
    "#### Connected Components Result\n",
    "\n",
    "1. `_curator_dedup_id` - The document IDs\n",
    "2. `_duplicate_group_id` - The group ID that document belongs to. Documents with the same duplicate group ID are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2549cfe9-3003-414c-a5cb-666e458b4615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>580</td>\n",
       "      <td>482048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582</td>\n",
       "      <td>320014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>588</td>\n",
       "      <td>320015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>590</td>\n",
       "      <td>482054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>592</td>\n",
       "      <td>482056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640509</th>\n",
       "      <td>2119663</td>\n",
       "      <td>479764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640510</th>\n",
       "      <td>2119665</td>\n",
       "      <td>159145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640511</th>\n",
       "      <td>2119668</td>\n",
       "      <td>320010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640512</th>\n",
       "      <td>2119672</td>\n",
       "      <td>320011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640513</th>\n",
       "      <td>2119674</td>\n",
       "      <td>320012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640514 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        _curator_dedup_id  _duplicate_group_id\n",
       "0                     580               482048\n",
       "1                     582               320014\n",
       "2                     588               320015\n",
       "3                     590               482054\n",
       "4                     592               482056\n",
       "...                   ...                  ...\n",
       "640509            2119663               479764\n",
       "640510            2119665               159145\n",
       "640511            2119668               320010\n",
       "640512            2119672               320011\n",
       "640513            2119674               320012\n",
       "\n",
       "[640514 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "_duplicate_group_id\n",
       "1              [197441, 577]\n",
       "4              [197445, 581]\n",
       "6              [197448, 584]\n",
       "7              [197449, 585]\n",
       "10             [197453, 589]\n",
       "                 ...        \n",
       "640504    [2119660, 1942709]\n",
       "640506    [1942713, 2119664]\n",
       "640507    [2119666, 1942715]\n",
       "640510    [1942719, 2119670]\n",
       "640513    [2119675, 1942724]\n",
       "Name: _curator_dedup_id, Length: 320043, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "_duplicate_group_id\n",
       "42263     230\n",
       "335749      3\n",
       "534437      3\n",
       "399046      3\n",
       "534446      3\n",
       "         ... \n",
       "238792      2\n",
       "426811      2\n",
       "426810      2\n",
       "426809      2\n",
       "316339      2\n",
       "Name: count, Length: 320043, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc_path = os.path.join(cache_path, \"ConnectedComponentsStage\")\n",
    "cc_df = pd.read_parquet(cc_path)  # works with pandas since the input here is small\n",
    "display(cc_df)\n",
    "grouped_cc_df = cc_df.groupby(\"_duplicate_group_id\")._curator_dedup_id.agg(list)\n",
    "display(grouped_cc_df)\n",
    "display(cc_df._duplicate_group_id.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87d677-40aa-49e1-bf6d-ab41a0941957",
   "metadata": {},
   "source": [
    "Based on the distribution above we can see that there is one cluster/group where 230 documents are all duplicates followed by many smaller clusters with 2/3 documents that are duplicates.\n",
    "\n",
    "#### FuzzyDuplicateIds Results (List of duplicate docs to remove)\n",
    "1. `_curator_dedup_id` - ID of docs in the removal list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda82d7a-8b35-4e64-8af0-5163bf1be875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id\n",
       "0                576\n",
       "1                579\n",
       "2                584\n",
       "3                585\n",
       "4                591"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate documents found for removal: 320471\n"
     ]
    }
   ],
   "source": [
    "duplicate_ids_path = os.path.join(fuzzy_output_dir, \"FuzzyDuplicateIds\")\n",
    "duplicates_df = pd.read_parquet(duplicate_ids_path)\n",
    "display(duplicates_df.head())\n",
    "\n",
    "print(f\"Number of duplicate documents found for removal: {len(duplicates_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af14853-2900-4d41-8792-6ff2b1cafb4c",
   "metadata": {},
   "source": [
    "#### Checking that the duplicate ids list contains only one document per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a1f8bf4-8e1b-425c-91f1-3d312bf17786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the duplicate group: 230\n",
      "Number of documents in the removal list from the same group: 229\n"
     ]
    }
   ],
   "source": [
    "# As an example let's look at the largest cluster corresponding to _duplicate_group_id `42263`\n",
    "duplicate_group = grouped_cc_df.loc[42263]\n",
    "\n",
    "# number of docs in the removal list from this group\n",
    "docs_to_remove_in_group = duplicates_df._curator_dedup_id.isin(duplicate_group).sum()\n",
    "\n",
    "print(f\"Number of documents in the duplicate group: {len(duplicate_group)}\")\n",
    "print(f\"Number of documents in the removal list from the same group: {docs_to_remove_in_group}\")\n",
    "assert docs_to_remove_in_group == (len(duplicate_group) - 1)  # noqa: S101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a6829-3566-4558-8b18-1fd5d0191d40",
   "metadata": {},
   "source": [
    "#### Advanced: Looking at examples of duplicate documents\n",
    "\n",
    "1. This analysis involves re-reading the input data with the same ID mapping that was used during duplicate identification.\n",
    "2. Merging the input data with the connected components results on the `_curator_dedup_id` column to associate each document which the duplicate group it belongs to which can be used for further analysis.\n",
    "\n",
    "NOTE: This analsis approach is itended for smaller datasets and only works for cases where the connected components dataframe is small and fits comfortable in memory. It is not recommended for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef243e5-b8df-47d5-b993-fa61a3954eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.pipeline import Pipeline\n",
    "from nemo_curator.stages.base import ProcessingStage\n",
    "from nemo_curator.stages.resources import Resources\n",
    "from nemo_curator.stages.text.io.reader import ParquetReader\n",
    "from nemo_curator.tasks.document import DocumentBatch\n",
    "\n",
    "\n",
    "class CustomMergeStage(ProcessingStage[DocumentBatch, DocumentBatch]):\n",
    "    \"\"\"\n",
    "    Warning: This should not be attempted with large connected components results.\n",
    "    A small stage that merges the input data (using the id's generated) with the connected components result.\n",
    "    Works because CC results are small enough to fit per batch.\n",
    "    \"\"\"\n",
    "\n",
    "    _resources = Resources(cpus=1.0)\n",
    "\n",
    "    def process(self, batch: DocumentBatch) -> DocumentBatch:\n",
    "        df = batch.to_pandas().merge(cc_df, how=\"inner\", on=[CURATOR_DEDUP_ID_STR])\n",
    "        return DocumentBatch(\n",
    "            task_id=batch.task_id, dataset_name=batch.dataset_name, data=df, _stage_perf=batch._stage_perf\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"Explore duplicates\",\n",
    "    stages=[ParquetReader(file_paths=input_path, blocksize=\"1GiB\", _assign_ids=True), CustomMergeStage()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44018e57-6748-4c39-aaf6-5caa2d1df247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 14:10:28,579\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:10:28,581\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "2025-11-18 14:10:28,581\tINFO worker.py:1851 -- Calling ray.init() again after it has already been called.\n",
      "2025-11-18 14:10:29,244\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:10:29,246\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "2025-11-18 14:10:29,252\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-11-18 14:10:29,266\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:10:29,267\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "2025-11-18 14:10:29,268\tINFO worker.py:1851 -- Calling ray.init() again after it has already been called.\n",
      "2025-11-18 14:10:54,658\tINFO worker.py:1692 -- Using address 127.0.1.1:6379 set in the environment variable RAY_ADDRESS\n",
      "2025-11-18 14:10:54,660\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6379...\n",
      "2025-11-18 14:10:54,666\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.core.client import RayClient\n",
    "from nemo_curator.stages.deduplication.id_generator import create_id_generator_actor, kill_id_generator_actor\n",
    "\n",
    "client = RayClient(num_cpus=8)  # change as needed\n",
    "client.start()\n",
    "\n",
    "create_id_generator_actor(filepath=os.path.join(fuzzy_output_dir, \"fuzzy_id_generator.json\"))\n",
    "merged_results = pipeline.run()\n",
    "merged_df = pd.concat([batch.to_pandas() for batch in merged_results]).sort_values(\"_duplicate_group_id\")\n",
    "kill_id_generator_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "070d42e8-28c2-4abd-a40f-a0fe103befbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115055</th>\n",
       "      <td></td>\n",
       "      <td>2c41b5de-fa15-4cd4-b802-68524a1ce611</td>\n",
       "      <td>387189</td>\n",
       "      <td>42263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433730</th>\n",
       "      <td></td>\n",
       "      <td>d928378b-b4e5-4795-97a1-584ae258df71</td>\n",
       "      <td>1437273</td>\n",
       "      <td>42263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556435</th>\n",
       "      <td></td>\n",
       "      <td>59011818-a780-41eb-97b0-abfa08a074f5</td>\n",
       "      <td>1852286</td>\n",
       "      <td>42263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556434</th>\n",
       "      <td></td>\n",
       "      <td>0cee9ce9-f5ce-4d6a-bf0e-bb9c14f9d972</td>\n",
       "      <td>1852285</td>\n",
       "      <td>42263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496191</th>\n",
       "      <td></td>\n",
       "      <td>57be64cd-f1af-4974-9034-9fbd3375e5e9</td>\n",
       "      <td>1657722</td>\n",
       "      <td>42263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369284</th>\n",
       "      <td></td>\n",
       "      <td>7c2f5029-2c35-496f-a885-92a51e92177d</td>\n",
       "      <td>1214999</td>\n",
       "      <td>42263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369285</th>\n",
       "      <td></td>\n",
       "      <td>dee264f7-a172-467d-866a-a596a3753be8</td>\n",
       "      <td>1215000</td>\n",
       "      <td>42263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587520</th>\n",
       "      <td></td>\n",
       "      <td>b03db266-da2d-425b-abda-bc59c30c24a3</td>\n",
       "      <td>1955797</td>\n",
       "      <td>42263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587512</th>\n",
       "      <td></td>\n",
       "      <td>9bae741c-aa83-414d-a081-1a75b1167361</td>\n",
       "      <td>1955789</td>\n",
       "      <td>42263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540427</th>\n",
       "      <td></td>\n",
       "      <td>62a95f0e-0d6a-4979-922f-18825064a97d</td>\n",
       "      <td>1803543</td>\n",
       "      <td>42263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text                                    id  _curator_dedup_id  \\\n",
       "115055       2c41b5de-fa15-4cd4-b802-68524a1ce611             387189   \n",
       "433730       d928378b-b4e5-4795-97a1-584ae258df71            1437273   \n",
       "556435       59011818-a780-41eb-97b0-abfa08a074f5            1852286   \n",
       "556434       0cee9ce9-f5ce-4d6a-bf0e-bb9c14f9d972            1852285   \n",
       "496191       57be64cd-f1af-4974-9034-9fbd3375e5e9            1657722   \n",
       "...     ...                                   ...                ...   \n",
       "369284       7c2f5029-2c35-496f-a885-92a51e92177d            1214999   \n",
       "369285       dee264f7-a172-467d-866a-a596a3753be8            1215000   \n",
       "587520       b03db266-da2d-425b-abda-bc59c30c24a3            1955797   \n",
       "587512       9bae741c-aa83-414d-a081-1a75b1167361            1955789   \n",
       "540427       62a95f0e-0d6a-4979-922f-18825064a97d            1803543   \n",
       "\n",
       "        _duplicate_group_id  \n",
       "115055                42263  \n",
       "433730                42263  \n",
       "556435                42263  \n",
       "556434                42263  \n",
       "496191                42263  \n",
       "...                     ...  \n",
       "369284                42263  \n",
       "369285                42263  \n",
       "587520                42263  \n",
       "587512                42263  \n",
       "540427                42263  \n",
       "\n",
       "[230 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(merged_df[merged_df._curator_dedup_id.isin(duplicate_group)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc423318-c1d5-4769-ba42-362f515c3166",
   "metadata": {},
   "source": [
    "The largest cluster/group of duplicates in this dataset seems to be documents with empty/no text.\n",
    "Looking through the next largest cluster here are the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a08dc31e-d4f9-47e0-b8b7-7f75e4e6abef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>255162</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "      <td>f03e9dfc-b61d-41a9-b816-cb99d3f77f3a</td>\n",
       "      <td>843160</td>\n",
       "      <td>335749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32337</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "      <td>a56ba800-0f22-4770-a2b6-19c39e8e5561</td>\n",
       "      <td>99566</td>\n",
       "      <td>335749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170410</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "      <td>2db7d179-9359-4c5a-ab5e-8112c9b11564</td>\n",
       "      <td>561484</td>\n",
       "      <td>335749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "255162  Once upon a time, there was a little girl name...   \n",
       "32337   Once upon a time, there was a little girl name...   \n",
       "170410  Once upon a time, there was a little girl name...   \n",
       "\n",
       "                                          id  _curator_dedup_id  \\\n",
       "255162  f03e9dfc-b61d-41a9-b816-cb99d3f77f3a             843160   \n",
       "32337   a56ba800-0f22-4770-a2b6-19c39e8e5561              99566   \n",
       "170410  2db7d179-9359-4c5a-ab5e-8112c9b11564             561484   \n",
       "\n",
       "        _duplicate_group_id  \n",
       "255162               335749  \n",
       "32337                335749  \n",
       "170410               335749  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document1\n",
      "----------\n",
      "Once upon a time, there was a little girl named Lily. She loved to eat spaghetti for dinner. One day, her mom made spaghetti with harmless tomato sauce. Lily was very happy and ate it all up. \n",
      "\n",
      "But then, her little brother came and spilled his juice all over the table. Lily sighed and her mom cleaned up the mess. After that, Lily played with her toys and went to bed. \n",
      "\n",
      "The next day, Lily's mom made spaghetti again. But this time, Lily didn't want to eat it. She sighed and said she wanted something else. Her mom made her a sandwich instead. The end.\n",
      "\n",
      "Document2\n",
      "----------\n",
      "Once upon a time, there was a little girl named Lily. She loved to eat spaghetti for dinner. One day, her mom made spaghetti with harmless tomato sauce. Lily was very happy and ate it all up. \n",
      "\n",
      "But then, her little brother came and spilled his juice all over the table. Lily sighed and her mom cleaned up the mess. After that, Lily played with her toys and went to bed. \n",
      "\n",
      "The next day, Lily's mom made spaghetti again. But this time, Lily didn't want to eat it. She sighed and said she wanted something else. Her mom made her a sandwich instead. The end.\n"
     ]
    }
   ],
   "source": [
    "duplicates = merged_df[merged_df._curator_dedup_id.isin(grouped_cc_df.loc[335749])]\n",
    "display(duplicates)\n",
    "\n",
    "print(f\"\\nDocument1\\n----------\\n{duplicates.iloc[0].text}\")\n",
    "print(f\"\\nDocument2\\n----------\\n{duplicates.iloc[1].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be9aca1d-f26a-4767-a9bc-9019969da01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a350d-3c5f-4e4e-8215-a5492f838c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
