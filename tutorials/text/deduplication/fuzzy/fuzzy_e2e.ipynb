{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe55f10a-8210-4746-9ddd-f7ad60a5cc52",
   "metadata": {},
   "source": [
    "# End-to-end Fuzzy Deduplication\n",
    "\n",
    "GPU accelerated implementation of a MinHash-LSH based fuzzy deduplication. For more information about Fuzzy deduplication in NeMo Curator, refer to the [Deduplication](https://docs.nvidia.com/nemo/curator/latest/curate-text/process-data/deduplication/index.html) section of the documentation page.\n",
    "\n",
    "The tutorial here shows how to run Fuzzy Duplication on text data by executing 2 end to end workflows.\n",
    "These 2 workflows roughly cover the following steps to perform fuzzy deduplication:\n",
    "\n",
    "1. Read original dataset\n",
    "2. Compute MinHashes signatures of these documents\n",
    "3. Perform LSH - Group Minhashes into bands/buckets and shuffle these bands/buckets so that documents in the same bucket are in the same batch/file.\n",
    "4. Convert the LSH outputs (bucket_id -> doc_id mapping) into an edgelist in preparation for connected components. \n",
    "5. Compute connected components across all potential duplicates found via LSH.\n",
    "6. Generate list of duplicate documents by randomly selecting 1 document to keep from each group/component and dropping the rest.\n",
    "7. Remove duplicates based on the generated duplicate list.\n",
    "\n",
    "We also allow users to also run these steps independently, which will be covered in the step by step tutorial in the same directory as this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4a272b-4ea6-4b03-9f57-be4cd16812cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fsspec\n",
    "\n",
    "# Silence Curator logs via Loguru\n",
    "os.environ[\"LOGURU_LEVEL\"] = \"INFO\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_dataset_path = \"./input\"  # Path to input dataset\n",
    "fuzzy_output_dir = \"./fuzzy_outputs\"  # Path to store all fuzzy outputs including cache & deduped dataset\n",
    "\n",
    "fuzzy_cache_path = os.path.join(\n",
    "    fuzzy_output_dir, \"cache\"\n",
    ")  # Path to store fuzzy deduplication intermediates (minhash, lsh etc.)\n",
    "deduplicated_output_path = os.path.join(fuzzy_output_dir, \"fuzzy_deduped_dataset\")\n",
    "\n",
    "input_filetype = (\n",
    "    \"parquet\"  # this can be either of jsonl or parquet (you'll need to change how input data is generated)\n",
    ")\n",
    "# Note: It's important that this is constant across identification and removal.\n",
    "# More information about choosing a good blocksize is mentioned in the performance considerations section below\n",
    "input_blocksize = \"512MiB\"\n",
    "output_filetype = \"parquet\"  # this can be either of jsonl or parquet\n",
    "\n",
    "storage_options = None  # Optional additional cloud I/O args to pass into Pandas/cuDF during I/O operations.\n",
    "fs, _ = fsspec.url_to_fs(fuzzy_cache_path, **storage_options if storage_options is not None else {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427f692-3e90-4ff8-9f90-a18f071182d6",
   "metadata": {},
   "source": [
    "### Downloading and saving a sample dataset\n",
    "\n",
    "We download and save the [Tinystories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset to the specified `input_dataset_path` above. This step can be skipped if running on a different dataset that's already present in the input_dataset_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f399d1-2486-4b8d-bd40-d3ddbd77dbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f2154233674a1280ba2fef47d836bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f88b1f884d4bfab75585fac69f8495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00004-2d5a1467fff108(…):   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e846960c484d9180b8d1665a3bf2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00004-5852b56a2bd28f(…):   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d1bdb567634effb50c19301da2a88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00004-a26307300439e9(…):   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147199333d82432eae5aa759067f785b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00004-d243063613e5a0(…):   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5460f2bce0467890086c866bbda099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001-869c898b5(…):   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33ba347c97948deba376449e91804a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda1f35f03aa43c8b9a74c2d62bdd85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0\n",
      "Processing file 50\n",
      "Processing file 100\n",
      "Processing file 150\n",
      "Processing file 200\n",
      "Created 212 files\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.utils.file_utils import get_all_file_paths_under\n",
    "\n",
    "if len(get_all_file_paths_under(input_dataset_path, storage_options=storage_options)) == 0:\n",
    "    import os\n",
    "    import uuid\n",
    "\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    input_df = load_dataset(\"roneneldan/TinyStories\", split=\"train\").to_pandas()\n",
    "    num_rows_per_file = 10_000\n",
    "\n",
    "    os.makedirs(input_dataset_path, exist_ok=True)\n",
    "\n",
    "    for i, start_idx in enumerate(range(0, len(input_df), num_rows_per_file)):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing file {i}\")\n",
    "        end_idx = min(len(input_df), start_idx + num_rows_per_file)\n",
    "        subset_df = input_df.iloc[start_idx:end_idx].copy()\n",
    "        subset_df[\"id\"] = [str(uuid.uuid4()) for _ in range(len(subset_df))]\n",
    "        subset_df.to_parquet(\n",
    "            os.path.join(input_dataset_path, f\"part_{i}.parquet\"), index=False, storage_options=storage_options\n",
    "        )\n",
    "\n",
    "    print(f\"Created {i + 1} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabcb72d-6d22-4324-a51c-73241f0d9d19",
   "metadata": {},
   "source": [
    "## Running as a Single Stage (End-to-End)\n",
    "\n",
    "See the [API Reference](https://docs.nvidia.com/nemo/curator/latest/apidocs/stages/stages.deduplication.fuzzy.workflow.html#api) for more information about the `FuzzyDeduplicationWorkflow` class.\n",
    "\n",
    "### General Notes\n",
    "#### ID Generation\n",
    "1. The ID generation process requires a Ray cluster to be started before running the workflow either from the CLI or by using the `RayClient` API in Curator.\n",
    "2. The Fuzzy Deduplication Workflow doesn't utilize any existing IDs in the input dataset and instead generates IDs on the fly using an ID Generator actor.\n",
    "3. The ID Generator gives each row a unique increasing integer ID, based on the order files are read.\n",
    "4. This avoids expensive ID->Integer encoding for the underlying connected components algorithm which only supports integer IDs.\n",
    "5. When we find duplicates, we save these integer IDs in sorted files with multiple row groups.\n",
    "6. We also save a `fuzzy_id_generator.json` which maintains a mapping of input file partitions to ID ranges for that batch.\n",
    "7. During removal, reading the same file groups will give the same integer IDs, using the min/max ID values, we can find all corresponding duplicates in that range making the process faster.\n",
    "\n",
    "#### Performance Considerations\n",
    "1. LSH - Configuring `bands_per_iteration` controls how many bands to process simultaneously in a single shuffle. Higher values can lead to faster performance but might increase memory pressure.\n",
    "2. A low `input_blocksize` may not saturate the GPUs enough while a high `input_blocksize` can lead to OOM errors during MinHash and excessive object store usage during removal. It's recommend to keep it at 512MiB-1.5GiB and reduce if running into OOMs during MinHash.\n",
    "3. The removal step can be memory intensive and it's recommend to set a higher fraction of object store memory for removal (if the machine has enough RAM). The `RayDataExecutor` showed better results during duplicate removal.\n",
    "4. The removal workflow is CPU only and can be run  on machines that don't have GPUs\n",
    "\n",
    "#### Hyperparameter Considerations\n",
    "1. The current defaults for fuzzy deduplication (260 hashes, 13 hashes per band) approximate finding documents with a Jaccard similarity of 0.8. For more information on selecting the number of bands/hashes it's recommended to analyze the S curve and tolerable threshold for false positives (and negatives). More information about LSH can be found in section `3.4.2` [here](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf).\n",
    "2. The `char_ngrams` values of 24 is set to approximate roughly ngrams that correspond to ~5 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf7a7a3-4659-45b3-be86-5803efb54f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.stages.deduplication.fuzzy import FuzzyDeduplicationWorkflow\n",
    "from nemo_curator.stages.deduplication.id_generator import CURATOR_DEDUP_ID_STR\n",
    "from nemo_curator.stages.text.deduplication import TextDuplicatesRemovalWorkflow\n",
    "\n",
    "# All workflows support passing in different kwargs and storage_options for the read, cache and output datasets\n",
    "# We use a common one here for simplicity\n",
    "io_kwargs = {\"storage_options\": storage_options} if storage_options is not None else None\n",
    "\n",
    "identification_workflow = FuzzyDeduplicationWorkflow(\n",
    "    cache_path=fuzzy_cache_path,\n",
    "    output_path=fuzzy_output_dir,\n",
    "    input_path=input_dataset_path,\n",
    "    input_filetype=input_filetype,\n",
    "    input_blocksize=input_blocksize,\n",
    "    text_field=\"text\",\n",
    "    seed=42,\n",
    "    char_ngrams=24,\n",
    "    minhashes_per_band=13,\n",
    "    bands_per_iteration=10,\n",
    "    read_kwargs=io_kwargs,\n",
    "    cache_kwargs=io_kwargs,\n",
    "    write_kwargs=io_kwargs,\n",
    ")\n",
    "\n",
    "removal_workflow = TextDuplicatesRemovalWorkflow(\n",
    "    input_path=input_dataset_path,  # Must be identical to the path used during identification\n",
    "    ids_to_remove_path=os.path.join(fuzzy_output_dir, \"FuzzyDuplicateIds\"),\n",
    "    output_path=deduplicated_output_path,\n",
    "    input_filetype=input_filetype,\n",
    "    input_blocksize=input_blocksize,  # This must be identical to the blocksize used during identification\n",
    "    ids_to_remove_duplicate_id_field=CURATOR_DEDUP_ID_STR,\n",
    "    id_generator_path=os.path.join(fuzzy_output_dir, \"fuzzy_id_generator.json\"),\n",
    "    output_filetype=output_filetype,\n",
    "    input_kwargs=io_kwargs,  # read_kwargs for input dataset\n",
    "    ids_to_remove_read_kwargs=io_kwargs,  # read_kwargs for removal_id's generated by Fuzzy workflow\n",
    "    id_generator_storage_options=storage_options,\n",
    "    output_kwargs=io_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7329ff2-3a36-42ad-833a-9589653d0e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-09 19:31:13.988\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnemo_curator.core.client\u001b[0m:\u001b[36mstart\u001b[0m:\u001b[36m106\u001b[0m - \u001b[33m\u001b[1mNo monitoring services are running. Please run the `start_prometheus_grafana.py` script from nemo_curator/metrics folder to setup monitoring services separately.\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:14.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.core.utils\u001b[0m:\u001b[36minit_cluster\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1mRay start command: ray start --head --node-ip-address 127.0.1.1 --port 6380 --metrics-export-port 8081 --dashboard-host 127.0.0.1 --dashboard-port 8267 --ray-client-server-port 20000 --temp-dir /tmp/ray --disable-usage-stats --num-gpus 2 --num-cpus 64 --block\u001b[0m\n",
      "2025-12-09 19:31:14,045\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 19:31:14,050\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "[2025-12-09 19:31:21,771 W 35875 35875] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-12-09 19:31:22,773 W 35875 35875] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-12-09 19:31:23,774 W 35875 35875] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-12-09 19:31:24,775 I 35875 35875] global_state_accessor.cc:487: This node has an IP address of 127.0.1.1, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.\n",
      "2025-12-09 19:31:24,778\tINFO worker.py:2012 -- Connected to Ray cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 19:31:16,858\tINFO usage_lib.py:447 -- Usage stats collection is disabled.\n",
      "2025-12-09 19:31:16,859\tINFO scripts.py:914 -- \u001b[37mLocal node IP\u001b[39m: \u001b[1m127.0.1.1\u001b[22m\n",
      "2025-12-09 19:31:24,784\tSUCC scripts.py:950 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-12-09 19:31:24,784\tSUCC scripts.py:951 -- \u001b[32mRay runtime started.\u001b[39m\n",
      "2025-12-09 19:31:24,784\tSUCC scripts.py:952 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-12-09 19:31:24,784\tINFO scripts.py:954 -- \u001b[36mNext steps\u001b[39m\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:957 -- To add another node to this Ray cluster, run\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:960 -- \u001b[1m  ray start --address='127.0.1.1:6380'\u001b[22m\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:969 -- To connect to this Ray cluster:\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:971 -- \u001b[35mimport\u001b[39m\u001b[26m ray\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:972 -- ray\u001b[35m.\u001b[39m\u001b[26minit(_node_ip_address\u001b[35m=\u001b[39m\u001b[26m\u001b[33m'127.0.1.1'\u001b[39m\u001b[26m)\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:984 -- To submit a Ray job using the Ray Jobs CLI:\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:985 -- \u001b[1m  RAY_API_SERVER_ADDRESS='http://127.0.0.1:8267' ray job submit --working-dir . -- python my_script.py\u001b[22m\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:994 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:998 -- for more information on submitting Ray jobs to the Ray cluster.\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:1003 -- To terminate the Ray runtime, run\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:1004 -- \u001b[1m  ray stop\u001b[22m\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:1007 -- To view the status of the cluster, use\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:1008 --   \u001b[1mray status\u001b[22m\u001b[26m\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:1012 -- To monitor and debug Ray, view the dashboard at \n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:1013 --   \u001b[1m127.0.0.1:8267\u001b[22m\u001b[26m\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:1020 -- \u001b[4mIf connection to the dashboard fails, check your firewall settings and network configuration.\u001b[24m\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:1121 -- \u001b[36m\u001b[1m--block\u001b[22m\u001b[39m\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:1122 -- This command will now block forever until terminated by a signal.\n",
      "2025-12-09 19:31:24,785\tINFO scripts.py:1125 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[32m2025-12-09 19:31:28.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.pipeline.pipeline\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mPlanning pipeline: minhash_pipeline\u001b[0m\n",
      "2025-12-09 19:31:28,299\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 19:31:28,303\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 19:31:28,309\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[32m2025-12-09 19:31:28.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.utils\u001b[0m:\u001b[36mexecute_setup_on_node\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mExecuting setup on node 6a76449a01128728abfd16efd066646e2a324805bc09b95fd625caca for 2 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:39.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSetup on node complete for all stages. Starting Ray Actor Pool pipeline with 2 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:39.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 1/2: FilePartitioningStage(file_paths='./input', files_per_partition=None, blocksize='512MiB', file_extensions=['.jsonl', '.json', '.parquet'], storage_options={}, limit=None, name='file_partitioning')\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:39.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 1\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:39.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=128, GPU limit=2147483647\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:39.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:39.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 0.5 CPUs, 0.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:39.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m FilePartitioningStage(file_paths='./input', files_per_partition=None, blocksize='512MiB', file_extensions=['.jsonl', '.json', '.parquet'], storage_options={}, limit=None, name='file_partitioning') - Creating 1 actors (CPUs: 0.5, GPUs: 0.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:39.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for file_partitioning with 1 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:43.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m290\u001b[0m - \u001b[1mBroke down 1 tasks into batches of 1 for a total of 1 batches for file_partitioning\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:43.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:43.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 2/2: MinHashStage\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:43.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 2\u001b[0m\n",
      "\u001b[36m(FilePartitioningStage pid=45958)\u001b[0m 2025-12-09 19:31:43.541 | INFO     | nemo_curator.stages.file_partitioning:process:95 - Found 212 files\n",
      "\u001b[36m(FilePartitioningStage pid=45958)\u001b[0m 2025-12-09 19:31:43.541 | INFO     | nemo_curator.stages.file_partitioning:process:132 - Created 2 file groups from 212 files\n",
      "\u001b[32m2025-12-09 19:31:43.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:43.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:43.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 1.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:43.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m MinHashStage - Creating 2 actors (CPUs: 1.0, GPUs: 1.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:43.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for MinHashStage with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:53.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m290\u001b[0m - \u001b[1mBroke down 2 tasks into batches of 1 for a total of 2 batches for MinHashStage\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:56.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:56.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1m\n",
      "Pipeline completed. Final results: 2 tasks\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:56.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mShutting down Ray to clean up all resources...\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:56.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mMinhash pipeline completed in 28.13 seconds\u001b[0m\n",
      "\u001b[32m2025-12-09 19:31:56.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.pipeline.pipeline\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mPlanning pipeline: lsh_duplicate_identification_pipeline\u001b[0m\n",
      "2025-12-09 19:31:56,434\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 19:31:56,438\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 19:31:56,444\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[32m2025-12-09 19:31:56.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.utils\u001b[0m:\u001b[36mexecute_setup_on_node\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mExecuting setup on node 6a76449a01128728abfd16efd066646e2a324805bc09b95fd625caca for 2 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:07.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSetup on node complete for all stages. Starting Ray Actor Pool pipeline with 2 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:07.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 1/2: FilePartitioningStage(file_paths='./fuzzy_outputs/cache/MinHashStage', files_per_partition=None, blocksize='2GiB', file_extensions=['.parquet'], storage_options={}, limit=None, name='file_partitioning')\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:07.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 1\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:07.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=128, GPU limit=2147483647\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:07.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:07.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 0.5 CPUs, 0.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:07.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m FilePartitioningStage(file_paths='./fuzzy_outputs/cache/MinHashStage', files_per_partition=None, blocksize='2GiB', file_extensions=['.parquet'], storage_options={}, limit=None, name='file_partitioning') - Creating 1 actors (CPUs: 0.5, GPUs: 0.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:07.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for file_partitioning with 1 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:11.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m290\u001b[0m - \u001b[1mBroke down 1 tasks into batches of 1 for a total of 1 batches for file_partitioning\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:11.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:11.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 2/2: LSHStage(num_bands=20, minhashes_per_band=13, id_field='_curator_dedup_id', minhash_field='_minhash_signature', output_path='./fuzzy_outputs/cache', read_kwargs={}, write_kwargs={}, rmm_pool_size='auto', spill_memory_limit='auto', enable_statistics=False, bands_per_iteration=10, total_nparts=None)\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:11.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:11.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m371\u001b[0m - \u001b[1m  Processing band range: 0-10\u001b[0m\n",
      "\u001b[36m(FilePartitioningStage pid=46722)\u001b[0m 2025-12-09 19:32:11.834 | INFO     | nemo_curator.stages.file_partitioning:process:95 - Found 2 files\n",
      "\u001b[36m(FilePartitioningStage pid=46722)\u001b[0m 2025-12-09 19:32:11.834 | INFO     | nemo_curator.stages.file_partitioning:process:132 - Created 2 file groups from 2 files\n",
      "\u001b[32m2025-12-09 19:32:12.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:12.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:12.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 1.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:12.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m383\u001b[0m - \u001b[1m LSHStage(num_bands=20, minhashes_per_band=13, id_field='_curator_dedup_id', minhash_field='_minhash_signature', output_path='./fuzzy_outputs/cache', read_kwargs={}, write_kwargs={}, rmm_pool_size='auto', spill_memory_limit='auto', enable_statistics=False, bands_per_iteration=10, total_nparts=None) - Creating 2 actors (CPUs: 1.0, GPUs: 1.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:12.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m387\u001b[0m - \u001b[1m  Creating RapidsMPFShuffling Actor Pool for stage: LSHStage\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:12.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1m    Initializing RapidsMPFShuffling actor pool with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:12.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m224\u001b[0m - \u001b[1m    Setting up UCXX communication...\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:22.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1m    UCXX setup complete\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:26.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m371\u001b[0m - \u001b[1m  Processing band range: 10-20\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:26.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:26.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:26.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 1.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:26.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m383\u001b[0m - \u001b[1m LSHStage(num_bands=20, minhashes_per_band=13, id_field='_curator_dedup_id', minhash_field='_minhash_signature', output_path='./fuzzy_outputs/cache', read_kwargs={}, write_kwargs={}, rmm_pool_size='auto', spill_memory_limit='auto', enable_statistics=False, bands_per_iteration=10, total_nparts=None) - Creating 2 actors (CPUs: 1.0, GPUs: 1.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:26.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m387\u001b[0m - \u001b[1m  Creating RapidsMPFShuffling Actor Pool for stage: LSHStage\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:26.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1m    Initializing RapidsMPFShuffling actor pool with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:26.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m224\u001b[0m - \u001b[1m    Setting up UCXX communication...\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:36.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1m    UCXX setup complete\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:40.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m396\u001b[0m - \u001b[1m  LSH processing complete. Output tasks: 4\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:40.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1m\n",
      "Pipeline completed. Final results: 4 tasks\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:40.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mShutting down Ray to clean up all resources...\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:40.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m315\u001b[0m - \u001b[1mLSH pipeline completed in 44.35 seconds\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:40.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.pipeline.pipeline\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mPlanning pipeline: connected_components_pipeline\u001b[0m\n",
      "2025-12-09 19:32:40,781\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 19:32:40,786\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 19:32:40,791\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[32m2025-12-09 19:32:40.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.utils\u001b[0m:\u001b[36mexecute_setup_on_node\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mExecuting setup on node 6a76449a01128728abfd16efd066646e2a324805bc09b95fd625caca for 3 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:48.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSetup on node complete for all stages. Starting Ray Actor Pool pipeline with 3 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:48.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 1/3: BucketsToEdgesStage\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:48.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 4\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:48.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2147483647\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:48.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:48.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 0.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:48.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m BucketsToEdgesStage - Creating 4 actors (CPUs: 1.0, GPUs: 0.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:48.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for BucketsToEdgesStage with 4 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 19:32:55.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m290\u001b[0m - \u001b[1mBroke down 4 tasks into batches of 1 for a total of 4 batches for BucketsToEdgesStage\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:03.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 4\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:03.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 2/3: ConnectedComponentsStage\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:03.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 4\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:03.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:03.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:03.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 1.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:03.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m ConnectedComponentsStage - Creating 2 actors (CPUs: 1.0, GPUs: 1.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:03.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1m  Creating RAFT actor pool for stage: ConnectedComponentsStage\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:03.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_raft_actor_pool\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1m    Initializing RAFT actor pool with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:03.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_raft_actor_pool\u001b[0m:\u001b[36m193\u001b[0m - \u001b[1m    Setting up RAFT communication...\u001b[0m\n",
      "\u001b[36m(ConnectedComponentsStage pid=48389)\u001b[0m 2025-12-09 19:33:11.730 | WARNING  | nemo_curator.backends.experimental.ray_actor_pool.raft_adapter:__init__:59 - batch size not set for stage ConnectedComponentsStage. Setting it to 1.\n",
      "\u001b[32m2025-12-09 19:33:13.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_raft_actor_pool\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1m    RAFT setup complete\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:13.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for ConnectedComponentsStage with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:13.962\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m276\u001b[0m - \u001b[33m\u001b[1mStage ConnectedComponentsStage is a RAFT stage but has a batch size of 1. Ignoring batch size.\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:13.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m286\u001b[0m - \u001b[1mDistributed 4 tasks evenly across 2 actors for RAFT stage ConnectedComponentsStage\u001b[0m\n",
      "\u001b[36m(ConnectedComponentsStage pid=48389)\u001b[0m 2025-12-09 19:33:16.389 | INFO     | nemo_curator.stages.deduplication.fuzzy.connected_components:weakly_connected_components:152 - Computing weakly connected components completed successfully!\n",
      "\u001b[32m2025-12-09 19:33:16.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:16.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 3/3: IdentifyDuplicatesStage\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:16.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:16.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:16.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:16.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 1.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:16.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m IdentifyDuplicatesStage - Creating 2 actors (CPUs: 1.0, GPUs: 1.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:16.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1m  Creating Shuffle actors for stage: IdentifyDuplicates\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:16.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1m    Initializing RapidsMPFShuffling actor pool with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:16.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m224\u001b[0m - \u001b[1m    Setting up UCXX communication...\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:26.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1m    UCXX setup complete\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:26.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for IdentifyDuplicates with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:30.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:30.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1m\n",
      "Pipeline completed. Final results: 2 tasks\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:30.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mShutting down Ray to clean up all resources...\u001b[0m\n",
      "\u001b[36m(ConnectedComponentsStage pid=48390)\u001b[0m 2025-12-09 19:33:11.737 | WARNING  | nemo_curator.backends.experimental.ray_actor_pool.raft_adapter:__init__:59 - batch size not set for stage ConnectedComponentsStage. Setting it to 1.\n",
      "\u001b[36m(ConnectedComponentsStage pid=48390)\u001b[0m 2025-12-09 19:33:16.389 | INFO     | nemo_curator.stages.deduplication.fuzzy.connected_components:weakly_connected_components:152 - Computing weakly connected components completed successfully!\n",
      "\u001b[32m2025-12-09 19:33:30.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1mConnected components pipeline completed in 50.15 seconds\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:30.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m333\u001b[0m - \u001b[1mNumber of documents removed: 320471\u001b[0m\n",
      "2025-12-09 19:33:30,936\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 19:33:30,940\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 19:33:30,947\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[32m2025-12-09 19:33:30.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mId generator written to ./fuzzy_outputs/fuzzy_id_generator.json\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:30.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m347\u001b[0m - \u001b[1mFuzzy deduplication pipeline completed in 122.68 seconds\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:30.980\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[33m\u001b[1mRay Data executor is experimental and might not work as expected.\u001b[0m\n",
      "2025-12-09 19:33:30,990\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 19:33:30,994\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 19:33:30,994\tINFO worker.py:1850 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[32m2025-12-09 19:33:31.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.pipeline.pipeline\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mPlanning pipeline: text_duplicates_removal_workflow\u001b[0m\n",
      "2025-12-09 19:33:31,565\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 19:33:31,569\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 19:33:31,576\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[32m2025-12-09 19:33:35.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.utils\u001b[0m:\u001b[36mexecute_setup_on_node\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mExecuting setup on node 6a76449a01128728abfd16efd066646e2a324805bc09b95fd625caca for 4 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:39.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mSetup on node complete for all stages. Starting Ray Data pipeline with 4 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:39.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mProcessing stage 1/4: FilePartitioningStage(file_paths='./input', files_per_partition=None, blocksize='512MiB', file_extensions=['.jsonl', '.json', '.parquet'], storage_options={}, limit=None, name='file_partitioning')\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:39.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1m  CPU cores: 0.5, GPU ratio: 0.0\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:39.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.adapter\u001b[0m:\u001b[36mprocess_dataset\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mFilePartitioningStage is_actor_stage_=False with concurrency_kwargs={'concurrency': None, 'num_cpus': 0.5}\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:39.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mProcessing stage 2/4: ParquetReaderStage(fields=None, read_kwargs=None, name='parquet_reader', _generate_ids=False, _assign_ids=True)\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:39.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1m  CPU cores: 1.0, GPU ratio: 0.0\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:40.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.adapter\u001b[0m:\u001b[36mprocess_dataset\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mParquetReaderStage is_actor_stage_=True with concurrency_kwargs={'concurrency': (1, 64), 'num_cpus': 1.0}\u001b[0m\n",
      "2025-12-09 19:33:40,012\tWARNING util.py:597 -- The argument ``concurrency`` is deprecated in Ray 2.51. Please specify argument ``compute`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "\u001b[32m2025-12-09 19:33:40.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mProcessing stage 3/4: TextDuplicatesRemovalStage(ids_to_remove_path='./fuzzy_outputs/FuzzyDuplicateIds', id_field='_curator_dedup_id', duplicate_id_field='_curator_dedup_id', read_kwargs={})\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:40.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1m  CPU cores: 1.0, GPU ratio: 0.0\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:40.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.adapter\u001b[0m:\u001b[36mprocess_dataset\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mTextDuplicatesRemovalStage is_actor_stage_=False with concurrency_kwargs={'concurrency': None, 'num_cpus': 1.0}\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:40.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mProcessing stage 4/4: ParquetWriter(path='./fuzzy_outputs/fuzzy_deduped_dataset', file_extension='parquet', write_kwargs={}, fields=None, name='parquet_writer', mode='ignore', append_mode_implemented=False)\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:40.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1m  CPU cores: 1.0, GPU ratio: 0.0\u001b[0m\n",
      "\u001b[32m2025-12-09 19:33:40.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.adapter\u001b[0m:\u001b[36mprocess_dataset\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mParquetWriter is_actor_stage_=False with concurrency_kwargs={'concurrency': None, 'num_cpus': 1.0}\u001b[0m\n",
      "2025-12-09 19:33:40,021\tINFO streaming_executor.py:85 -- A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True`.\n",
      "2025-12-09 19:33:40,021\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_5_0\n",
      "2025-12-09 19:33:40,034\tINFO streaming_executor.py:170 -- Starting execution of Dataset dataset_5_0. Full logs are in /tmp/ray/session_2025-12-09_19-31-16_860328_36393/logs/ray-data\n",
      "2025-12-09 19:33:40,034\tINFO streaming_executor.py:171 -- Execution plan of Dataset dataset_5_0: InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(FilePartitioningStageTask)] -> TaskPoolMapOperator[StreamingRepartition] -> ActorPoolMapOperator[MapBatches(ParquetReaderStageActor)] -> TaskPoolMapOperator[MapBatches(TextDuplicatesRemovalStageTask)->MapBatches(ParquetWriterTask)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8935875773e40889116edd0b9ffa406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa60aca0522241699a0912bb82a44595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(FilePartitioningStageTask) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0a3c6765414caaa433f1854368a724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- StreamingRepartition 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca1b2505ba84c1e9953adf154868dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(ParquetReaderStageActor) 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94060c3b1f0c434693bd2c0e15755283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(TextDuplicatesRemovalStageTask)->MapBatches(ParquetWriterTask) 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 19:33:40,108\tWARNING resource_manager.py:134 -- ⚠️  Ray's object store is configured to use only 10.2% of available memory (186.3GiB out of 1824.0GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "\u001b[36m(MapBatches(FilePartitioningStageTask) pid=49184)\u001b[0m 2025-12-09 19:33:40.352 | INFO     | nemo_curator.stages.file_partitioning:process:95 - Found 212 files\n",
      "\u001b[36m(MapBatches(FilePartitioningStageTask) pid=49184)\u001b[0m 2025-12-09 19:33:40.352 | INFO     | nemo_curator.stages.file_partitioning:process:132 - Created 2 file groups from 212 files\n",
      "2025-12-09 19:34:02,812\tINFO streaming_executor.py:298 -- ✔️  Dataset dataset_5_0 execution finished in 22.78 seconds\n",
      "2025-12-09 19:34:02,818\tINFO util.py:257 -- Exiting prefetcher's background thread\n",
      "\u001b[32m2025-12-09 19:34:02.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mPipeline completed. Final results: 2 tasks\u001b[0m\n",
      "2025-12-09 19:34:03,106\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 19:34:03,111\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 19:34:03,117\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.backends.experimental.ray_data import RayDataExecutor\n",
    "from nemo_curator.core.client import RayClient\n",
    "\n",
    "client = RayClient(num_cpus=64, num_gpus=2)  # change as needed\n",
    "client.start()\n",
    "\n",
    "_ = identification_workflow.run()\n",
    "_ = removal_workflow.run(executor=RayDataExecutor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c983f3-f562-45e6-bd09-5660cb179aa3",
   "metadata": {},
   "source": [
    "### Looking at Intermediate Results and Output\n",
    "\n",
    "#### MinHash Results\n",
    "1. `_curator_dedup_id` - The IDs assigned to this dataset on the fly during the initial read.\n",
    "2. `_minhash_signature` - MinHash Signature\n",
    "\n",
    "#### LSH Results\n",
    "1. `_bucket_id` - The bucket/band identifier\n",
    "2. `_curator_dedup_id` - List of all document IDs that belong to that bucket\n",
    "\n",
    "#### Buckets To Edges Result\n",
    "1. `_curator_dedup_id_x`, `_curator_dedup_id_y` - Mapping of edges in a Graph where each column are documents that are potential duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d9b66f-0d48-4a90-96d6-570fd218777c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_minhash_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1110000</td>\n",
       "      <td>[7605662, 4337118, 6484429, 8346452, 6444847, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1110001</td>\n",
       "      <td>[2938900, 4353439, 3290251, 1074973, 4860283, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1110002</td>\n",
       "      <td>[129864, 3231856, 1046776, 1777482, 4032304, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1110003</td>\n",
       "      <td>[8395592, 14856053, 2666175, 3533855, 18487142...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1110004</td>\n",
       "      <td>[265861, 19780184, 11961285, 16232798, 1259680...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id                                 _minhash_signature\n",
       "0            1110000  [7605662, 4337118, 6484429, 8346452, 6444847, ...\n",
       "1            1110001  [2938900, 4353439, 3290251, 1074973, 4860283, ...\n",
       "2            1110002  [129864, 3231856, 1046776, 1777482, 4032304, 1...\n",
       "3            1110003  [8395592, 14856053, 2666175, 3533855, 18487142...\n",
       "4            1110004  [265861, 19780184, 11961285, 16232798, 1259680..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_bucket_id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0_000055fd7daae1e46223e8b7e06bf2e0</td>\n",
       "      <td>[1178375, 2079489]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0_0000f975e5bcda25838df43b0d37737f</td>\n",
       "      <td>[965853, 1334885]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0_0001c9dff36e10d709d64123cb0dee4d</td>\n",
       "      <td>[299769, 1936007]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b0_00020b2c889483bd6a78ffe9a8d7deb1</td>\n",
       "      <td>[261169, 2008278]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b0_00024c2b7321353410dd908eb31499bd</td>\n",
       "      <td>[213076, 990707]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _bucket_id   _curator_dedup_id\n",
       "0  b0_000055fd7daae1e46223e8b7e06bf2e0  [1178375, 2079489]\n",
       "1  b0_0000f975e5bcda25838df43b0d37737f   [965853, 1334885]\n",
       "2  b0_0001c9dff36e10d709d64123cb0dee4d   [299769, 1936007]\n",
       "3  b0_00020b2c889483bd6a78ffe9a8d7deb1   [261169, 2008278]\n",
       "4  b0_00024c2b7321353410dd908eb31499bd    [213076, 990707]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id_x</th>\n",
       "      <th>_curator_dedup_id_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128422</td>\n",
       "      <td>1370422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>568235</td>\n",
       "      <td>1552660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>873929</td>\n",
       "      <td>2098065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>997647</td>\n",
       "      <td>1289916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1309916</td>\n",
       "      <td>1952254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id_x  _curator_dedup_id_y\n",
       "0               128422              1370422\n",
       "1               568235              1552660\n",
       "2               873929              2098065\n",
       "3               997647              1289916\n",
       "4              1309916              1952254"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minhash_path = os.path.join(fuzzy_cache_path, \"MinHashStage\")\n",
    "display(pd.read_parquet(fs.unstrip_protocol(fs.find(minhash_path)[0]), storage_options=storage_options).head())\n",
    "\n",
    "lsh_path = os.path.join(fuzzy_cache_path, \"LSHStage\")\n",
    "display(pd.read_parquet(fs.unstrip_protocol(fs.find(lsh_path)[0]), storage_options=storage_options).head())\n",
    "\n",
    "b2e_path = os.path.join(fuzzy_cache_path, \"BucketsToEdgesStage\")\n",
    "display(pd.read_parquet(fs.unstrip_protocol(fs.find(b2e_path)[0]), storage_options=storage_options).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650db74e-b764-44fa-966a-e4f0ddcb7182",
   "metadata": {},
   "source": [
    "#### Connected Components Result\n",
    "\n",
    "1. `_curator_dedup_id` - The document IDs\n",
    "2. `_duplicate_group_id` - The group ID that document belongs to. Documents with the same duplicate group ID are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2549cfe9-3003-414c-a5cb-666e458b4615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>580</td>\n",
       "      <td>482276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582</td>\n",
       "      <td>320167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>588</td>\n",
       "      <td>482280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>590</td>\n",
       "      <td>320169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>592</td>\n",
       "      <td>320170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640509</th>\n",
       "      <td>2119663</td>\n",
       "      <td>480120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640510</th>\n",
       "      <td>2119665</td>\n",
       "      <td>320162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640511</th>\n",
       "      <td>2119668</td>\n",
       "      <td>320163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640512</th>\n",
       "      <td>2119672</td>\n",
       "      <td>158947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640513</th>\n",
       "      <td>2119674</td>\n",
       "      <td>320165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640514 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        _curator_dedup_id  _duplicate_group_id\n",
       "0                     580               482276\n",
       "1                     582               320167\n",
       "2                     588               482280\n",
       "3                     590               320169\n",
       "4                     592               320170\n",
       "...                   ...                  ...\n",
       "640509            2119663               480120\n",
       "640510            2119665               320162\n",
       "640511            2119668               320163\n",
       "640512            2119672               158947\n",
       "640513            2119674               320165\n",
       "\n",
       "[640514 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "_duplicate_group_id\n",
       "0              [576, 187440]\n",
       "2              [578, 187442]\n",
       "3              [579, 187443]\n",
       "4              [581, 187445]\n",
       "6              [584, 187448]\n",
       "                 ...        \n",
       "640508    [2119667, 1942716]\n",
       "640509    [2119669, 1942718]\n",
       "640511    [1942720, 2119671]\n",
       "640512    [1942722, 2119673]\n",
       "640513    [2119675, 1942724]\n",
       "Name: _curator_dedup_id, Length: 320043, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "_duplicate_group_id\n",
       "142559    230\n",
       "505715      3\n",
       "436273      3\n",
       "88121       3\n",
       "88124       3\n",
       "         ... \n",
       "551430      2\n",
       "426966      2\n",
       "551428      2\n",
       "426964      2\n",
       "159712      2\n",
       "Name: count, Length: 320043, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc_path = os.path.join(fuzzy_cache_path, \"ConnectedComponentsStage\")\n",
    "cc_df = pd.read_parquet(cc_path, storage_options=storage_options)  # works with pandas since the input here is small\n",
    "display(cc_df)\n",
    "grouped_cc_df = cc_df.groupby(\"_duplicate_group_id\")._curator_dedup_id.agg(list)\n",
    "display(grouped_cc_df)\n",
    "duplicate_cluster_sizes = cc_df._duplicate_group_id.value_counts()\n",
    "display(duplicate_cluster_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87d677-40aa-49e1-bf6d-ab41a0941957",
   "metadata": {},
   "source": [
    "Based on the distribution above we can see that there is one cluster/group where 230 documents are all duplicates followed by many smaller clusters with 2 or 3 documents that are duplicates.\n",
    "\n",
    "#### FuzzyDuplicateIds Results (List of duplicate docs to remove)\n",
    "1. `_curator_dedup_id` - ID of docs in the removal list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda82d7a-8b35-4e64-8af0-5163bf1be875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id\n",
       "0                594\n",
       "1                607\n",
       "2                614\n",
       "3                625\n",
       "4                632"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate documents found for removal: 320471\n"
     ]
    }
   ],
   "source": [
    "duplicate_ids_path = os.path.join(fuzzy_output_dir, \"FuzzyDuplicateIds\")\n",
    "duplicates_df = pd.read_parquet(duplicate_ids_path, storage_options=storage_options)\n",
    "display(duplicates_df.head())\n",
    "\n",
    "print(f\"Number of duplicate documents found for removal: {len(duplicates_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af14853-2900-4d41-8792-6ff2b1cafb4c",
   "metadata": {},
   "source": [
    "#### Checking that the duplicate ids list contains only one document per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a1f8bf4-8e1b-425c-91f1-3d312bf17786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the duplicate group: 230\n",
      "Number of documents in the removal list from the same group: 229\n"
     ]
    }
   ],
   "source": [
    "# As an example let's look at the group with the largest number of duplicates\n",
    "largest_duplicate_cluster = grouped_cc_df.loc[duplicate_cluster_sizes.index[0]]\n",
    "\n",
    "# number of docs in the removal list from this group\n",
    "docs_to_remove_in_group = duplicates_df._curator_dedup_id.isin(largest_duplicate_cluster).sum()\n",
    "\n",
    "print(f\"Number of documents in the duplicate group: {len(largest_duplicate_cluster)}\")\n",
    "print(f\"Number of documents in the removal list from the same group: {docs_to_remove_in_group}\")\n",
    "assert docs_to_remove_in_group == (len(largest_duplicate_cluster) - 1)  # noqa: S101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a6829-3566-4558-8b18-1fd5d0191d40",
   "metadata": {},
   "source": [
    "#### Advanced: Looking at examples of duplicate documents\n",
    "\n",
    "1. This analysis involves re-reading the input data with the same ID mapping that was used during duplicate identification.\n",
    "2. Merging the input data with the connected components results on the `_curator_dedup_id` column to associate each document which the duplicate group it belongs to which can be used for further analysis.\n",
    "\n",
    "**NOTE**: This analysis approach is intended as an example for smaller datasets and only works for cases where the connected components dataframe is small and fits comfortable in memory. It is not recommended for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef243e5-b8df-47d5-b993-fa61a3954eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.pipeline import Pipeline\n",
    "from nemo_curator.stages.base import ProcessingStage\n",
    "from nemo_curator.stages.resources import Resources\n",
    "from nemo_curator.stages.text.io.reader import JsonlReader, ParquetReader\n",
    "from nemo_curator.tasks.document import DocumentBatch\n",
    "\n",
    "\n",
    "class CustomMergeStage(ProcessingStage[DocumentBatch, DocumentBatch]):\n",
    "    \"\"\"\n",
    "    Warning: This should not be attempted with large connected components results.\n",
    "    A small stage that merges the input data (using the id's generated) with the connected components result.\n",
    "    Works because CC results are small enough to fit per batch.\n",
    "    \"\"\"\n",
    "\n",
    "    resources = Resources(cpus=1.0)\n",
    "\n",
    "    def process(self, batch: DocumentBatch) -> DocumentBatch:\n",
    "        df = batch.to_pandas().merge(cc_df, how=\"inner\", on=[CURATOR_DEDUP_ID_STR])\n",
    "        return DocumentBatch(\n",
    "            task_id=batch.task_id, dataset_name=batch.dataset_name, data=df, _stage_perf=batch._stage_perf\n",
    "        )\n",
    "\n",
    "\n",
    "ReaderClass = ParquetReader if input_filetype == \"parquet\" else JsonlReader\n",
    "pipeline = Pipeline(\n",
    "    name=\"Explore duplicates\",\n",
    "    stages=[\n",
    "        ReaderClass(file_paths=input_dataset_path, blocksize=input_blocksize, _assign_ids=True, read_kwargs=io_kwargs),\n",
    "        CustomMergeStage(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44018e57-6748-4c39-aaf6-5caa2d1df247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.core.client import RayClient\n",
    "from nemo_curator.stages.deduplication.id_generator import create_id_generator_actor, kill_id_generator_actor\n",
    "\n",
    "try:\n",
    "    create_id_generator_actor(\n",
    "        filepath=os.path.join(fuzzy_output_dir, \"fuzzy_id_generator.json\"), storage_options=storage_options\n",
    "    )\n",
    "    merged_results = pipeline.run()\n",
    "    merged_df = pd.concat([batch.to_pandas() for batch in merged_results]).sort_values(\"_duplicate_group_id\")\n",
    "finally:\n",
    "    kill_id_generator_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "070d42e8-28c2-4abd-a40f-a0fe103befbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289502</th>\n",
       "      <td></td>\n",
       "      <td>1084d700-f756-4dca-a198-f3b6414909e1</td>\n",
       "      <td>952302</td>\n",
       "      <td>142559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200157</th>\n",
       "      <td></td>\n",
       "      <td>777cc06f-29fc-47ef-ae25-27a1ee7af9f8</td>\n",
       "      <td>1795067</td>\n",
       "      <td>142559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92644</th>\n",
       "      <td></td>\n",
       "      <td>900e74e2-b609-4fb8-be0c-0ecb2c593cc6</td>\n",
       "      <td>305150</td>\n",
       "      <td>142559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200156</th>\n",
       "      <td></td>\n",
       "      <td>662b6000-0f0c-465a-b075-c7dfcb403f91</td>\n",
       "      <td>1795066</td>\n",
       "      <td>142559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200155</th>\n",
       "      <td></td>\n",
       "      <td>577a7853-11d8-4a35-a96f-2a3725c5f446</td>\n",
       "      <td>1795065</td>\n",
       "      <td>142559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330564</th>\n",
       "      <td></td>\n",
       "      <td>c7cf48e1-0053-47a3-8a2b-8f66ad659fbd</td>\n",
       "      <td>1083898</td>\n",
       "      <td>142559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31037</th>\n",
       "      <td></td>\n",
       "      <td>c92fe911-f72f-4e01-b609-fb2801eafd75</td>\n",
       "      <td>1214995</td>\n",
       "      <td>142559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30803</th>\n",
       "      <td></td>\n",
       "      <td>cf7e6d4b-ed55-4c50-96d1-8b25de87bed6</td>\n",
       "      <td>1213140</td>\n",
       "      <td>142559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95464</th>\n",
       "      <td></td>\n",
       "      <td>9bb7152f-eb03-477e-b77b-92fc064903d7</td>\n",
       "      <td>1437250</td>\n",
       "      <td>142559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30804</th>\n",
       "      <td></td>\n",
       "      <td>fb0b74b5-f9b2-4c10-8462-5728ef2f27bd</td>\n",
       "      <td>1213141</td>\n",
       "      <td>142559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text                                    id  _curator_dedup_id  \\\n",
       "289502       1084d700-f756-4dca-a198-f3b6414909e1             952302   \n",
       "200157       777cc06f-29fc-47ef-ae25-27a1ee7af9f8            1795067   \n",
       "92644        900e74e2-b609-4fb8-be0c-0ecb2c593cc6             305150   \n",
       "200156       662b6000-0f0c-465a-b075-c7dfcb403f91            1795066   \n",
       "200155       577a7853-11d8-4a35-a96f-2a3725c5f446            1795065   \n",
       "...     ...                                   ...                ...   \n",
       "330564       c7cf48e1-0053-47a3-8a2b-8f66ad659fbd            1083898   \n",
       "31037        c92fe911-f72f-4e01-b609-fb2801eafd75            1214995   \n",
       "30803        cf7e6d4b-ed55-4c50-96d1-8b25de87bed6            1213140   \n",
       "95464        9bb7152f-eb03-477e-b77b-92fc064903d7            1437250   \n",
       "30804        fb0b74b5-f9b2-4c10-8462-5728ef2f27bd            1213141   \n",
       "\n",
       "        _duplicate_group_id  \n",
       "289502               142559  \n",
       "200157               142559  \n",
       "92644                142559  \n",
       "200156               142559  \n",
       "200155               142559  \n",
       "...                     ...  \n",
       "330564               142559  \n",
       "31037                142559  \n",
       "30803                142559  \n",
       "95464                142559  \n",
       "30804                142559  \n",
       "\n",
       "[230 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(merged_df[merged_df._curator_dedup_id.isin(largest_duplicate_cluster)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc423318-c1d5-4769-ba42-362f515c3166",
   "metadata": {},
   "source": [
    "The largest cluster/group of duplicates in this dataset seems to be all documents with empty/no text.\n",
    "\n",
    "Let's look at the second largest cluster of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a08dc31e-d4f9-47e0-b8b7-7f75e4e6abef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>255165</th>\n",
       "      <td>Once upon a time, there was a band of animals....</td>\n",
       "      <td>301ec69c-e5ef-4da8-81bd-b9d426526ef3</td>\n",
       "      <td>843163</td>\n",
       "      <td>505715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170413</th>\n",
       "      <td>Once upon a time, there was a band of animals....</td>\n",
       "      <td>b2412c63-121e-4579-b881-4a922ccc4b08</td>\n",
       "      <td>561487</td>\n",
       "      <td>505715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32340</th>\n",
       "      <td>Once upon a time, there was a band of animals....</td>\n",
       "      <td>8cb47acc-0f61-4da2-b88c-02f7fbdb0b21</td>\n",
       "      <td>99569</td>\n",
       "      <td>505715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "255165  Once upon a time, there was a band of animals....   \n",
       "170413  Once upon a time, there was a band of animals....   \n",
       "32340   Once upon a time, there was a band of animals....   \n",
       "\n",
       "                                          id  _curator_dedup_id  \\\n",
       "255165  301ec69c-e5ef-4da8-81bd-b9d426526ef3             843163   \n",
       "170413  b2412c63-121e-4579-b881-4a922ccc4b08             561487   \n",
       "32340   8cb47acc-0f61-4da2-b88c-02f7fbdb0b21              99569   \n",
       "\n",
       "        _duplicate_group_id  \n",
       "255165               505715  \n",
       "170413               505715  \n",
       "32340                505715  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document1\n",
      "----------\n",
      "Once upon a time, there was a band of animals. They were all friends and loved to play music together. The band had a cat who played the guitar, a dog who played the drums, and a bird who sang.\n",
      "\n",
      "One day, the band got into a big fight. The cat was impatient and wanted to play a fast song, but the dog wanted to play a slow song. They argued and argued until they couldn't play together anymore.\n",
      "\n",
      "The bird missed playing with the band and decided to do something about it. She went to each animal and talked to them. She told the cat to be patient and let the dog play a slow song. She told the dog to try playing a faster beat. Finally, the band united and played the best song they ever played before. They were all happy and played together every day after that.\n",
      "\n",
      "Document2\n",
      "----------\n",
      "Once upon a time, there was a band of animals. They were all friends and loved to play music together. The band had a cat who played the guitar, a dog who played the drums, and a bird who sang.\n",
      "\n",
      "One day, the band got into a big fight. The cat was impatient and wanted to play a fast song, but the dog wanted to play a slow song. They argued and argued until they couldn't play together anymore.\n",
      "\n",
      "The bird missed playing with the band and decided to do something about it. She went to each animal and talked to them. She told the cat to be patient and let the dog play a slow song. She told the dog to try playing a faster beat. Finally, the band united and played the best song they ever played before. They were all happy and played together every day after that.\n"
     ]
    }
   ],
   "source": [
    "duplicates = merged_df[merged_df._curator_dedup_id.isin(grouped_cc_df.loc[duplicate_cluster_sizes.index[1]])]\n",
    "display(duplicates)\n",
    "\n",
    "print(f\"\\nDocument1\\n----------\\n{duplicates.iloc[0].text}\")\n",
    "print(f\"\\nDocument2\\n----------\\n{duplicates.iloc[1].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be9aca1d-f26a-4767-a9bc-9019969da01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-09 20:16:52.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.core.client\u001b[0m:\u001b[36mstop\u001b[0m:\u001b[36m181\u001b[0m - \u001b[1mNeMo Curator has stopped the Ray cluster it started by killing the Ray GCS process. It is advised to wait for a few seconds before running any Ray commands to ensure Ray can cleanup other processes.If you are seeing any Ray commands like `ray status` failing, please ensure /tmp/ray/ray_current_cluster has correct information.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "client.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016ea8c-26b0-451b-b436-09ebdef9a5df",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We were able to find and remove ~320_000 duplicate documents in a dataset of ~2.1 Million Rows "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
