{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe55f10a-8210-4746-9ddd-f7ad60a5cc52",
   "metadata": {},
   "source": [
    "# End-to-end Fuzzy Deduplication\n",
    "\n",
    "GPU accelerated implementation of a MinHash-LSH based fuzzy deduplication. For more information about Fuzzy deduplication in NeMo Curator, refer to the [Deduplication](https://docs.nvidia.com/nemo/curator/latest/curate-text/process-data/deduplication/index.html) section of the documentation page.\n",
    "\n",
    "The tutorial here shows how to run Fuzzy Duplication on text data by executing a 2 end to end workflows which does the following:\n",
    "\n",
    "1. Read original dataset\n",
    "2. Compute MinHashes signatures of these documents\n",
    "3. Perform LSH - Group Minhashes into bands/buckets and shuffle these bands/buckets so that documents in the same bucket are in the same batch/file.\n",
    "4. Convert the LSH outputs (bucket_id -> doc_id mapping) into a edgelist in preperation for connected components. \n",
    "5. Compute connected components across all potential duplicates found via LSH.\n",
    "6. Generate list of duplicate documents by randomly selecting 1 document to keep from each group/component and dropping the rest.\n",
    "7. Remove duplicates based on the generated duplicate list.\n",
    "\n",
    "We also allow users to also run these steps independently, which will be covered in the step by step tutorial in the same directory as this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4a272b-4ea6-4b03-9f57-be4cd16812cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Silence Curator logs via Loguru\n",
    "os.environ[\"LOGURU_LEVEL\"] = \"ERROR\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_dataset_path = os.path.abspath(\"./input\")  # Path to input dataset\n",
    "fuzzy_output_dir = os.path.abspath(\"./output/e2e\")  # Path to store all fuzzy outputs including cache & deduped dataset\n",
    "fuzzy_cache_path = os.path.join(\n",
    "    fuzzy_output_dir, \"cache\"\n",
    ")  # Path to store fuzzy deduplication intermediates (minhash, lsh etc.)\n",
    "deduplicated_output_path = os.path.join(fuzzy_output_dir, \"fuzzy_deduped_dataset\")\n",
    "\n",
    "input_filetype = (\n",
    "    \"parquet\"  # this can be either of jsonl or parquet (you'll need to change how input data is generated)\n",
    ")\n",
    "output_filetype = \"parquet\"  # this can be either of jsonl or parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f399d1-2486-4b8d-bd40-d3ddbd77dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.utils.file_utils import get_all_file_paths_under\n",
    "\n",
    "if len(get_all_file_paths_under(input_dataset_path)) == 0:\n",
    "    import os\n",
    "    import uuid\n",
    "\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    input_df = load_dataset(\"roneneldan/TinyStories\", split=\"train\").to_pandas()\n",
    "    num_rows_per_file = 10_000\n",
    "\n",
    "    os.makedirs(input_dataset_path, exist_ok=True)\n",
    "\n",
    "    for i, start_idx in enumerate(range(0, len(input_df), num_rows_per_file)):\n",
    "        end_idx = min(len(input_df), start_idx + num_rows_per_file)\n",
    "        subset_df = input_df.iloc[start_idx:end_idx].copy()\n",
    "        subset_df[\"id\"] = [str(uuid.uuid4()) for _ in range(len(subset_df))]\n",
    "        subset_df.to_parquet(os.path.join(input_dataset_path, f\"part_{i}.parquet\"), index=False)\n",
    "\n",
    "    print(f\"Created {len(os.listdir(input_dataset_path))} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabcb72d-6d22-4324-a51c-73241f0d9d19",
   "metadata": {},
   "source": [
    "## Running as a Single Stage (End-to-End)\n",
    "\n",
    "See the [API Reference](https://docs.nvidia.com/nemo/curator/latest/apidocs/stages/stages.deduplication.fuzzy.workflow.html#api) for more information about the `FuzzyDeduplicationWorkflow` class.\n",
    "\n",
    "### General Notes\n",
    "#### ID Generation\n",
    "1. The Fuzzy Deduplication Workflow doesn't utilize any existing IDs in the input dataset and instead generates IDs on the fly using an ID Generator actor.\n",
    "2. The ID Generator gives each row a unique increasing integer ID, based on the order files are read.\n",
    "3. This avoids expensive ID->Integer encoding for the underlying connected components algorithm which only supports integer IDs.\n",
    "4. When we find duplicates, we save these integer IDs in sorted files with multiple row groups.\n",
    "5. We also save a `fuzzy_id_generator.json` which maintains a mapping of input file partitions to ID ranges for that batch.\n",
    "6. During removal, reading the same file groups will give the same integer IDs, using the min/max ID values, we can find all corresponding duplicates in that range making the process faster.\n",
    "\n",
    "#### Performance Considerations\n",
    "1. LSH - Configuring bands_per_iteration controls how many bands to process simultanesouly in a single shuffle. Higher values can lead to faster performance but might increase memory pressure.\n",
    "2. A low `input_blocksize` may not saturate the GPUs enough while a high `input_blocksize` can lead to OOM errors during MinHash and excessive object store usage during removal. It's recommend to keep it at 1-1.5GiB and reduce if running into OOMs during MinHash.\n",
    "3. The removal step can be memory intensive and it's recommend to set a higher fraction of object store memory for removal (if the machine has enough RAM). The `RayDataExecutor` showed better results during duplicate removal.\n",
    "4. The removal workflow is CPU only and can be run  on machines that don't have GPUs\n",
    "\n",
    "#### Hyperparameter Considerations\n",
    "1. The current defaults for Fuzzy deduplication (260 hashes, 13 hashes per band) approximate finding documents with a jaccard similarity of 0.8. For more information on selecting the number of bands/hashes it's recommended to analyze the S curve and tolerable threshold for false postives (and negatives). More information about LSH can be found in section `3.4.2` [here](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf).\n",
    "2. The `char_ngrams` values of 24 is set to approximate roughly ngrams that correspond to ~5 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf7a7a3-4659-45b3-be86-5803efb54f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.stages.deduplication.fuzzy import FuzzyDeduplicationWorkflow\n",
    "from nemo_curator.stages.deduplication.id_generator import CURATOR_DEDUP_ID_STR\n",
    "from nemo_curator.stages.text.deduplication import TextDuplicatesRemovalWorkflow\n",
    "\n",
    "identification_workflow = FuzzyDeduplicationWorkflow(\n",
    "    cache_path=fuzzy_cache_path,\n",
    "    output_path=fuzzy_output_dir,\n",
    "    input_path=input_dataset_path,\n",
    "    input_filetype=input_filetype,\n",
    "    input_blocksize=\"1GiB\",\n",
    "    text_field=\"text\",\n",
    "    seed=42,\n",
    "    char_ngrams=24,\n",
    "    minhashes_per_band=13,\n",
    "    bands_per_iteration=10,\n",
    ")\n",
    "\n",
    "removal_workflow = TextDuplicatesRemovalWorkflow(\n",
    "    input_path=input_dataset_path,  # Must be identical to the path used during identification\n",
    "    ids_to_remove_path=os.path.join(fuzzy_output_dir, \"FuzzyDuplicateIds\"),\n",
    "    output_path=deduplicated_output_path,\n",
    "    input_filetype=input_filetype,\n",
    "    input_blocksize=\"1GiB\",  # This must be identical to the blocksize used during identification\n",
    "    ids_to_remove_duplicate_id_field=CURATOR_DEDUP_ID_STR,\n",
    "    id_generator_path=os.path.join(fuzzy_output_dir, \"fuzzy_id_generator.json\"),\n",
    "    output_filetype=\"parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7329ff2-3a36-42ad-833a-9589653d0e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 09:13:20,334\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:13:20,336\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "[2025-11-20 09:13:25,485 W 2843400 2843400] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-11-20 09:13:26,486 W 2843400 2843400] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-11-20 09:13:27,487 W 2843400 2843400] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "2025-11-20 09:13:27,613\tINFO utils.py:87 -- Overwriting previous Ray address (127.0.1.1:6381). Running ray.init() on this node will now connect to the new instance at 127.0.1.1:6382. To override this behavior, pass address=127.0.1.1:6381 to ray.init().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-20 09:13:22,775\tINFO usage_lib.py:447 -- Usage stats collection is disabled.\n",
      "2025-11-20 09:13:22,776\tINFO scripts.py:914 -- \u001b[37mLocal node IP\u001b[39m: \u001b[1m127.0.1.1\u001b[22m\n",
      "2025-11-20 09:13:27,612\tSUCC scripts.py:950 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-11-20 09:13:27,612\tSUCC scripts.py:951 -- \u001b[32mRay runtime started.\u001b[39m\n",
      "2025-11-20 09:13:27,612\tSUCC scripts.py:952 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-11-20 09:13:27,612\tINFO scripts.py:954 -- \u001b[36mNext steps\u001b[39m\n",
      "2025-11-20 09:13:27,612\tINFO scripts.py:957 -- To add another node to this Ray cluster, run\n",
      "2025-11-20 09:13:27,612\tINFO scripts.py:960 -- \u001b[1m  ray start --address='127.0.1.1:6382'\u001b[22m\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:969 -- To connect to this Ray cluster:\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:971 -- \u001b[35mimport\u001b[39m\u001b[26m ray\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:972 -- ray\u001b[35m.\u001b[39m\u001b[26minit(_node_ip_address\u001b[35m=\u001b[39m\u001b[26m\u001b[33m'127.0.1.1'\u001b[39m\u001b[26m)\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:984 -- To submit a Ray job using the Ray Jobs CLI:\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:985 -- \u001b[1m  RAY_API_SERVER_ADDRESS='http://127.0.0.1:8268' ray job submit --working-dir . -- python my_script.py\u001b[22m\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:994 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:998 -- for more information on submitting Ray jobs to the Ray cluster.\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:1003 -- To terminate the Ray runtime, run\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:1004 -- \u001b[1m  ray stop\u001b[22m\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:1007 -- To view the status of the cluster, use\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:1008 --   \u001b[1mray status\u001b[22m\u001b[26m\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:1012 -- To monitor and debug Ray, view the dashboard at \n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:1013 --   \u001b[1m127.0.0.1:8268\u001b[22m\u001b[26m\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:1020 -- \u001b[4mIf connection to the dashboard fails, check your firewall settings and network configuration.\u001b[24m\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:1121 -- \u001b[36m\u001b[1m--block\u001b[22m\u001b[39m\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:1122 -- This command will now block forever until terminated by a signal.\n",
      "2025-11-20 09:13:27,613\tINFO scripts.py:1125 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 09:13:28,488 I 2843400 2843400] global_state_accessor.cc:487: This node has an IP address of 127.0.1.1, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.\n",
      "2025-11-20 09:13:28,491\tINFO worker.py:2013 -- Connected to Ray cluster.\n",
      "/raid/adattagupta/NeMo-Curator/.venv/lib/python3.12/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "2025-11-20 09:13:32,986\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:13:32,988\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "2025-11-20 09:13:32,994\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8268 \u001b[39m\u001b[22m\n",
      "2025-11-20 09:13:53,154\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:13:53,156\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "2025-11-20 09:13:53,161\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8268 \u001b[39m\u001b[22m\n",
      "2025-11-20 09:14:17,596\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:14:17,598\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "2025-11-20 09:14:17,604\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8268 \u001b[39m\u001b[22m\n",
      "2025-11-20 09:15:06,138\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:15:06,140\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "2025-11-20 09:15:06,145\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8268 \u001b[39m\u001b[22m\n",
      "2025-11-20 09:15:06,174\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:15:06,175\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "2025-11-20 09:15:06,175\tINFO worker.py:1851 -- Calling ray.init() again after it has already been called.\n",
      "2025-11-20 09:15:06,656\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:15:06,658\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "2025-11-20 09:15:06,664\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8268 \u001b[39m\u001b[22m\n",
      "2025-11-20 09:15:13,739\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_5_0\n",
      "2025-11-20 09:15:13,751\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_5_0. Full logs are in /tmp/ray/session_2025-11-20_09-13-22_776593_2856946/logs/ray-data\n",
      "2025-11-20 09:15:13,751\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_5_0: InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(FilePartitioningStageTask)] -> TaskPoolMapOperator[StreamingRepartition] -> ActorPoolMapOperator[MapBatches(ParquetReaderStageActor)] -> TaskPoolMapOperator[MapBatches(TextDuplicatesRemovalStageTask)->MapBatches(ParquetWriterTask)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6590f50be04b83a6ea183efb158bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 09:15:13,792 E 2843400 2843400] core_worker.cc:2153: Actor with class name: 'MapWorker(MapBatches(ParquetReaderStageActor))' and ID: 'c956838f83654fc601f9e42f06000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576aaa645df74c2f8b5fdfd9825614ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(FilePartitioningStageTask) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85590f9d5544557b41609aa8cdfe32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- StreamingRepartition 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0e80d90fcb4589b00e299e32a000ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(ParquetReaderStageActor) 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68cdc1ed3d34ba0b4bb3188eef66955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(TextDuplicatesRemovalStageTask)->MapBatches(ParquetWriterTask) 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 09:15:13,819\tWARNING resource_manager.py:134 -- ⚠️  Ray's object store is configured to use only 12.0% of available memory (186.3GiB out of 1555.2GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2025-11-20 09:15:42,543\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_5_0 execution finished in 28.79 seconds\n",
      "2025-11-20 09:15:42,592\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:15:42,593\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "2025-11-20 09:15:42,599\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8268 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.backends.experimental.ray_data import RayDataExecutor\n",
    "from nemo_curator.core.client import RayClient\n",
    "\n",
    "client = RayClient(num_cpus=64, num_gpus=2)  # change as needed\n",
    "client.start()\n",
    "\n",
    "_ = identification_workflow.run()\n",
    "_ = removal_workflow.run(executor=RayDataExecutor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c983f3-f562-45e6-bd09-5660cb179aa3",
   "metadata": {},
   "source": [
    "### Looking at Intermediate Results and Output\n",
    "\n",
    "#### MinHash Results\n",
    "1. `_curator_dedup_id` - The IDs assigned to this dataset on the fly during the intial read.\n",
    "2. `_minhash_signature` - MinHash Signature\n",
    "\n",
    "#### LSH Results\n",
    "1. `_bucket_id` - The bucket/band identifier\n",
    "2. `_curator_dedup_id` - List of all document IDs that belong to that bucket\n",
    "\n",
    "#### Buckets To Edges Result\n",
    "1. `_curator_dedup_id_x`, `_curator_dedup_id_y` - Mapping of edges in a Graph where each column are documents that are potential duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d9b66f-0d48-4a90-96d6-570fd218777c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_minhash_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[11644717, 429172, 6014805, 86354, 2387151, 49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[2103321, 653305, 2941429, 5780991, 6977799, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[1891498, 3797631, 2961751, 50078, 21382505, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[1286357, 4060996, 1376561, 3044837, 7369355, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[6272013, 12535265, 819579, 5975720, 25677928,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id                                 _minhash_signature\n",
       "0                  0  [11644717, 429172, 6014805, 86354, 2387151, 49...\n",
       "1                  1  [2103321, 653305, 2941429, 5780991, 6977799, 7...\n",
       "2                  2  [1891498, 3797631, 2961751, 50078, 21382505, 5...\n",
       "3                  3  [1286357, 4060996, 1376561, 3044837, 7369355, ...\n",
       "4                  4  [6272013, 12535265, 819579, 5975720, 25677928,..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_bucket_id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0_000055fd7daae1e46223e8b7e06bf2e0</td>\n",
       "      <td>[1158375, 2079489]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0_00006a5f30f7b2c96588bfc1bfb5321a</td>\n",
       "      <td>[365218, 1933514]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0_00006f316d5bd251bd83702e3f1e017f</td>\n",
       "      <td>[161590, 771961]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b0_0000d90e9e4140a7ac31e6b227a62f62</td>\n",
       "      <td>[8290, 567169]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b0_0000f975e5bcda25838df43b0d37737f</td>\n",
       "      <td>[965853, 1334885]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _bucket_id   _curator_dedup_id\n",
       "0  b0_000055fd7daae1e46223e8b7e06bf2e0  [1158375, 2079489]\n",
       "1  b0_00006a5f30f7b2c96588bfc1bfb5321a   [365218, 1933514]\n",
       "2  b0_00006f316d5bd251bd83702e3f1e017f    [161590, 771961]\n",
       "3  b0_0000d90e9e4140a7ac31e6b227a62f62      [8290, 567169]\n",
       "4  b0_0000f975e5bcda25838df43b0d37737f   [965853, 1334885]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id_x</th>\n",
       "      <th>_curator_dedup_id_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150383</td>\n",
       "      <td>921559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679249</td>\n",
       "      <td>1126071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128422</td>\n",
       "      <td>1370422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29885</td>\n",
       "      <td>516200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270728</td>\n",
       "      <td>1089921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id_x  _curator_dedup_id_y\n",
       "0               150383               921559\n",
       "1               679249              1126071\n",
       "2               128422              1370422\n",
       "3                29885               516200\n",
       "4               270728              1089921"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minhash_path = os.path.join(fuzzy_cache_path, \"MinHashStage\")\n",
    "display(pd.read_parquet(os.path.join(minhash_path, os.listdir(minhash_path)[0])).head())\n",
    "\n",
    "lsh_path = os.path.join(fuzzy_cache_path, \"LSHStage\")\n",
    "display(pd.read_parquet(os.path.join(lsh_path, os.listdir(lsh_path)[0])).head())\n",
    "\n",
    "b2e_path = os.path.join(fuzzy_cache_path, \"BucketsToEdgesStage\")\n",
    "display(pd.read_parquet(os.path.join(b2e_path, os.listdir(b2e_path)[0])).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650db74e-b764-44fa-966a-e4f0ddcb7182",
   "metadata": {},
   "source": [
    "#### Connected Components Result\n",
    "\n",
    "1. `_curator_dedup_id` - The document IDs\n",
    "2. `_duplicate_group_id` - The group ID that document belongs to. Documents with the same duplicate group ID are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2549cfe9-3003-414c-a5cb-666e458b4615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>577</td>\n",
       "      <td>482206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>579</td>\n",
       "      <td>482207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>581</td>\n",
       "      <td>482209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640509</th>\n",
       "      <td>2119669</td>\n",
       "      <td>640509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640510</th>\n",
       "      <td>2119670</td>\n",
       "      <td>640510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640511</th>\n",
       "      <td>2119671</td>\n",
       "      <td>479940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640512</th>\n",
       "      <td>2119673</td>\n",
       "      <td>479941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640513</th>\n",
       "      <td>2119675</td>\n",
       "      <td>640513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640514 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        _curator_dedup_id  _duplicate_group_id\n",
       "0                     576                    0\n",
       "1                     577               482206\n",
       "2                     578                    2\n",
       "3                     579               482207\n",
       "4                     581               482209\n",
       "...                   ...                  ...\n",
       "640509            2119669               640509\n",
       "640510            2119670               640510\n",
       "640511            2119671               479940\n",
       "640512            2119673               479941\n",
       "640513            2119675               640513\n",
       "\n",
       "[640514 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "_duplicate_group_id\n",
       "0              [576, 197440]\n",
       "2              [578, 197442]\n",
       "5              [583, 197447]\n",
       "6              [584, 197448]\n",
       "7              [585, 197449]\n",
       "                 ...        \n",
       "640507    [1942715, 2119666]\n",
       "640508    [1942716, 2119667]\n",
       "640509    [1942718, 2119669]\n",
       "640510    [1942719, 2119670]\n",
       "640513    [1942724, 2119675]\n",
       "Name: _curator_dedup_id, Length: 320043, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "_duplicate_group_id\n",
       "463522    230\n",
       "504398      3\n",
       "88059       3\n",
       "88060       3\n",
       "559408      3\n",
       "         ... \n",
       "551514      2\n",
       "106803      2\n",
       "514494      2\n",
       "514493      2\n",
       "480738      2\n",
       "Name: count, Length: 320043, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc_path = os.path.join(fuzzy_cache_path, \"ConnectedComponentsStage\")\n",
    "cc_df = pd.read_parquet(cc_path)  # works with pandas since the input here is small\n",
    "display(cc_df)\n",
    "grouped_cc_df = cc_df.groupby(\"_duplicate_group_id\")._curator_dedup_id.agg(list)\n",
    "display(grouped_cc_df)\n",
    "duplicate_cluster_sizes = cc_df._duplicate_group_id.value_counts()\n",
    "display(duplicate_cluster_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87d677-40aa-49e1-bf6d-ab41a0941957",
   "metadata": {},
   "source": [
    "Based on the distribution above we can see that there is one cluster/group where 230 documents are all duplicates followed by many smaller clusters with 2/3 documents that are duplicates.\n",
    "\n",
    "#### FuzzyDuplicateIds Results (List of duplicate docs to remove)\n",
    "1. `_curator_dedup_id` - ID of docs in the removal list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda82d7a-8b35-4e64-8af0-5163bf1be875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id\n",
       "0                577\n",
       "1                579\n",
       "2                581\n",
       "3                584\n",
       "4                585"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate documents found for removal: 320471\n"
     ]
    }
   ],
   "source": [
    "duplicate_ids_path = os.path.join(fuzzy_output_dir, \"FuzzyDuplicateIds\")\n",
    "duplicates_df = pd.read_parquet(duplicate_ids_path)\n",
    "display(duplicates_df.head())\n",
    "\n",
    "print(f\"Number of duplicate documents found for removal: {len(duplicates_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af14853-2900-4d41-8792-6ff2b1cafb4c",
   "metadata": {},
   "source": [
    "#### Checking that the duplicate ids list contains only one document per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a1f8bf4-8e1b-425c-91f1-3d312bf17786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the duplicate group: 230\n",
      "Number of documents in the removal list from the same group: 229\n"
     ]
    }
   ],
   "source": [
    "# As an example let's look at the group with the largest number of duplicates\n",
    "largest_duplicate_cluster = grouped_cc_df.loc[duplicate_cluster_sizes.index[0]]\n",
    "\n",
    "# number of docs in the removal list from this group\n",
    "docs_to_remove_in_group = duplicates_df._curator_dedup_id.isin(largest_duplicate_cluster).sum()\n",
    "\n",
    "print(f\"Number of documents in the duplicate group: {len(largest_duplicate_cluster)}\")\n",
    "print(f\"Number of documents in the removal list from the same group: {docs_to_remove_in_group}\")\n",
    "assert docs_to_remove_in_group == (len(largest_duplicate_cluster) - 1)  # noqa: S101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a6829-3566-4558-8b18-1fd5d0191d40",
   "metadata": {},
   "source": [
    "#### Advanced: Looking at examples of duplicate documents\n",
    "\n",
    "1. This analysis involves re-reading the input data with the same ID mapping that was used during duplicate identification.\n",
    "2. Merging the input data with the connected components results on the `_curator_dedup_id` column to associate each document which the duplicate group it belongs to which can be used for further analysis.\n",
    "\n",
    "**NOTE**: This analsis approach is itended as an example for smaller datasets and only works for cases where the connected components dataframe is small and fits comfortable in memory. It is not recommended for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef243e5-b8df-47d5-b993-fa61a3954eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.pipeline import Pipeline\n",
    "from nemo_curator.stages.base import ProcessingStage\n",
    "from nemo_curator.stages.resources import Resources\n",
    "from nemo_curator.stages.text.io.reader import ParquetReader\n",
    "from nemo_curator.tasks.document import DocumentBatch\n",
    "\n",
    "\n",
    "class CustomMergeStage(ProcessingStage[DocumentBatch, DocumentBatch]):\n",
    "    \"\"\"\n",
    "    Warning: This should not be attempted with large connected components results.\n",
    "    A small stage that merges the input data (using the id's generated) with the connected components result.\n",
    "    Works because CC results are small enough to fit per batch.\n",
    "    \"\"\"\n",
    "\n",
    "    _resources = Resources(cpus=1.0)\n",
    "\n",
    "    def process(self, batch: DocumentBatch) -> DocumentBatch:\n",
    "        df = batch.to_pandas().merge(cc_df, how=\"inner\", on=[CURATOR_DEDUP_ID_STR])\n",
    "        return DocumentBatch(\n",
    "            task_id=batch.task_id, dataset_name=batch.dataset_name, data=df, _stage_perf=batch._stage_perf\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"Explore duplicates\",\n",
    "    stages=[ParquetReader(file_paths=input_dataset_path, blocksize=\"1GiB\", _assign_ids=True), CustomMergeStage()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44018e57-6748-4c39-aaf6-5caa2d1df247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 09:22:33,038\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:22:33,039\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "2025-11-20 09:22:33,039\tINFO worker.py:1851 -- Calling ray.init() again after it has already been called.\n",
      "2025-11-20 09:22:33,745\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:22:33,747\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "2025-11-20 09:22:33,752\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8268 \u001b[39m\u001b[22m\n",
      "2025-11-20 09:22:33,768\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:22:33,769\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "2025-11-20 09:22:33,769\tINFO worker.py:1851 -- Calling ray.init() again after it has already been called.\n",
      "2025-11-20 09:23:00,724\tINFO worker.py:1692 -- Using address 127.0.1.1:6382 set in the environment variable RAY_ADDRESS\n",
      "2025-11-20 09:23:00,726\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 127.0.1.1:6382...\n",
      "2025-11-20 09:23:00,732\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8268 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.core.client import RayClient\n",
    "from nemo_curator.stages.deduplication.id_generator import create_id_generator_actor, kill_id_generator_actor\n",
    "\n",
    "client = RayClient(num_cpus=8)  # change as needed\n",
    "client.start()\n",
    "\n",
    "create_id_generator_actor(filepath=os.path.join(fuzzy_output_dir, \"fuzzy_id_generator.json\"))\n",
    "merged_results = pipeline.run()\n",
    "merged_df = pd.concat([batch.to_pandas() for batch in merged_results]).sort_values(\"_duplicate_group_id\")\n",
    "kill_id_generator_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "070d42e8-28c2-4abd-a40f-a0fe103befbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>369299</th>\n",
       "      <td></td>\n",
       "      <td>b8f7e80f-5c03-4686-8502-36c93874fe23</td>\n",
       "      <td>1215014</td>\n",
       "      <td>463522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93041</th>\n",
       "      <td></td>\n",
       "      <td>1c2dfd9b-7f03-468d-9d1c-d53848129834</td>\n",
       "      <td>304132</td>\n",
       "      <td>463522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258686</th>\n",
       "      <td></td>\n",
       "      <td>d466d372-2468-428a-9b4e-b0db0e545e36</td>\n",
       "      <td>856047</td>\n",
       "      <td>463522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369307</th>\n",
       "      <td></td>\n",
       "      <td>49eba958-d175-4969-9eee-87ca0b42ab5e</td>\n",
       "      <td>1215084</td>\n",
       "      <td>463522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557145</th>\n",
       "      <td></td>\n",
       "      <td>f5d0e2d8-b5ba-4996-b44b-2da667e26807</td>\n",
       "      <td>1855005</td>\n",
       "      <td>463522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557151</th>\n",
       "      <td></td>\n",
       "      <td>9e493d0e-c479-4465-9c35-431765d36484</td>\n",
       "      <td>1855012</td>\n",
       "      <td>463522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369309</th>\n",
       "      <td></td>\n",
       "      <td>96a15122-9f50-41ab-8911-59afee17af60</td>\n",
       "      <td>1215088</td>\n",
       "      <td>463522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192032</th>\n",
       "      <td></td>\n",
       "      <td>573e97d5-2229-457d-9e41-b98f4ee901b5</td>\n",
       "      <td>630284</td>\n",
       "      <td>463522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368142</th>\n",
       "      <td></td>\n",
       "      <td>7bc9641f-4e2e-4771-ae40-0f12603a7695</td>\n",
       "      <td>1208884</td>\n",
       "      <td>463522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369312</th>\n",
       "      <td></td>\n",
       "      <td>2903d9bf-a823-4649-a2d1-278a9e22e8d2</td>\n",
       "      <td>1215095</td>\n",
       "      <td>463522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text                                    id  _curator_dedup_id  \\\n",
       "369299       b8f7e80f-5c03-4686-8502-36c93874fe23            1215014   \n",
       "93041        1c2dfd9b-7f03-468d-9d1c-d53848129834             304132   \n",
       "258686       d466d372-2468-428a-9b4e-b0db0e545e36             856047   \n",
       "369307       49eba958-d175-4969-9eee-87ca0b42ab5e            1215084   \n",
       "557145       f5d0e2d8-b5ba-4996-b44b-2da667e26807            1855005   \n",
       "...     ...                                   ...                ...   \n",
       "557151       9e493d0e-c479-4465-9c35-431765d36484            1855012   \n",
       "369309       96a15122-9f50-41ab-8911-59afee17af60            1215088   \n",
       "192032       573e97d5-2229-457d-9e41-b98f4ee901b5             630284   \n",
       "368142       7bc9641f-4e2e-4771-ae40-0f12603a7695            1208884   \n",
       "369312       2903d9bf-a823-4649-a2d1-278a9e22e8d2            1215095   \n",
       "\n",
       "        _duplicate_group_id  \n",
       "369299               463522  \n",
       "93041                463522  \n",
       "258686               463522  \n",
       "369307               463522  \n",
       "557145               463522  \n",
       "...                     ...  \n",
       "557151               463522  \n",
       "369309               463522  \n",
       "192032               463522  \n",
       "368142               463522  \n",
       "369312               463522  \n",
       "\n",
       "[230 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(merged_df[merged_df._curator_dedup_id.isin(largest_duplicate_cluster)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc423318-c1d5-4769-ba42-362f515c3166",
   "metadata": {},
   "source": [
    "The largest cluster/group of duplicates in this dataset seems to be all documents with empty/no text.\n",
    "\n",
    "Let's look at the second largest cluster of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a08dc31e-d4f9-47e0-b8b7-7f75e4e6abef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32338</th>\n",
       "      <td>Once upon a time, there was a little boy named...</td>\n",
       "      <td>b44cee45-e69d-4c30-b1d8-8ba04996d9a3</td>\n",
       "      <td>99567</td>\n",
       "      <td>504398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248446</th>\n",
       "      <td>Once upon a time, there was a little boy named...</td>\n",
       "      <td>3b551a23-3588-490c-ba03-941ea481e33c</td>\n",
       "      <td>823161</td>\n",
       "      <td>504398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170411</th>\n",
       "      <td>Once upon a time, there was a little boy named...</td>\n",
       "      <td>2a7ccbf2-97da-4bb9-9906-6c019e9af8e6</td>\n",
       "      <td>561485</td>\n",
       "      <td>504398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "32338   Once upon a time, there was a little boy named...   \n",
       "248446  Once upon a time, there was a little boy named...   \n",
       "170411  Once upon a time, there was a little boy named...   \n",
       "\n",
       "                                          id  _curator_dedup_id  \\\n",
       "32338   b44cee45-e69d-4c30-b1d8-8ba04996d9a3              99567   \n",
       "248446  3b551a23-3588-490c-ba03-941ea481e33c             823161   \n",
       "170411  2a7ccbf2-97da-4bb9-9906-6c019e9af8e6             561485   \n",
       "\n",
       "        _duplicate_group_id  \n",
       "32338                504398  \n",
       "248446               504398  \n",
       "170411               504398  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document1\n",
      "----------\n",
      "Once upon a time, there was a little boy named Timmy. Timmy loved to play with his toy cars and make them go zoom. One day, Timmy saw some big kids playing basketball. Timmy wanted to play too, but the big kids said he was too young. Timmy felt frustrated and sad. \n",
      "\n",
      "Timmy went home and told his mom about what happened. His mom said, \"Timmy, just because you are young doesn't mean you can't play. You should ask the big kids if you can play with them.\" \n",
      "\n",
      "The next day, Timmy went back to the basketball court and asked the big kids if he could play. They said yes! Timmy was so happy and he played with the big kids all day. But as the sun began to set, Timmy's ball accidentally hit a car and the owner of the car got very angry. Timmy didn't know what to do and he felt scared. \n",
      "\n",
      "The moral of the story is that sometimes things don't go the way we want them to, but we should always try our best and be responsible for our actions.\n",
      "\n",
      "Document2\n",
      "----------\n",
      "Once upon a time, there was a little boy named Timmy. Timmy loved to play with his toy cars and make them go zoom. One day, Timmy saw some big kids playing basketball. Timmy wanted to play too, but the big kids said he was too young. Timmy felt frustrated and sad. \n",
      "\n",
      "Timmy went home and told his mom about what happened. His mom said, \"Timmy, just because you are young doesn't mean you can't play. You should ask the big kids if you can play with them.\" \n",
      "\n",
      "The next day, Timmy went back to the basketball court and asked the big kids if he could play. They said yes! Timmy was so happy and he played with the big kids all day. But as the sun began to set, Timmy's ball accidentally hit a car and the owner of the car got very angry. Timmy didn't know what to do and he felt scared. \n",
      "\n",
      "The moral of the story is that sometimes things don't go the way we want them to, but we should always try our best and be responsible for our actions.\n"
     ]
    }
   ],
   "source": [
    "duplicates = merged_df[merged_df._curator_dedup_id.isin(grouped_cc_df.loc[duplicate_cluster_sizes.index[1]])]\n",
    "display(duplicates)\n",
    "\n",
    "print(f\"\\nDocument1\\n----------\\n{duplicates.iloc[0].text}\")\n",
    "print(f\"\\nDocument2\\n----------\\n{duplicates.iloc[1].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be9aca1d-f26a-4767-a9bc-9019969da01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad449646-6ecc-4a86-9b9c-16bd05bff332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
