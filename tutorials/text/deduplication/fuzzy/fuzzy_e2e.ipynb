{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe55f10a-8210-4746-9ddd-f7ad60a5cc52",
   "metadata": {},
   "source": [
    "# End-to-end Fuzzy Deduplication\n",
    "\n",
    "GPU accelerated implementation of a MinHash-LSH based fuzzy deduplication. For more information about Fuzzy deduplication in NeMo Curator, refer to the [Deduplication](https://docs.nvidia.com/nemo/curator/latest/curate-text/process-data/deduplication/index.html) section of the documentation page.\n",
    "\n",
    "The tutorial here shows how to run Fuzzy Duplication on text data by executing 2 end to end workflows.\n",
    "These 2 workflows roughly cover the following steps to perform fuzzy deduplication:\n",
    "\n",
    "1. Read original dataset\n",
    "2. Compute MinHashes signatures of these documents\n",
    "3. Perform LSH - Group Minhashes into bands/buckets and shuffle these bands/buckets so that documents in the same bucket are in the same batch/file.\n",
    "4. Convert the LSH outputs (bucket_id -> doc_id mapping) into a edgelist in preparation for connected components. \n",
    "5. Compute connected components across all potential duplicates found via LSH.\n",
    "6. Generate list of duplicate documents by randomly selecting 1 document to keep from each group/component and dropping the rest.\n",
    "7. Remove duplicates based on the generated duplicate list.\n",
    "\n",
    "We also allow users to also run these steps independently, which will be covered in the step by step tutorial in the same directory as this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4a272b-4ea6-4b03-9f57-be4cd16812cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fsspec\n",
    "\n",
    "# Silence Curator logs via Loguru\n",
    "os.environ[\"LOGURU_LEVEL\"] = \"INFO\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_dataset_path = \"./input\"  # Path to input dataset\n",
    "fuzzy_output_dir = \"./fuzzy_outputs\"  # Path to store all fuzzy outputs including cache & deduped dataset\n",
    "\n",
    "fuzzy_cache_path = os.path.join(\n",
    "    fuzzy_output_dir, \"cache\"\n",
    ")  # Path to store fuzzy deduplication intermediates (minhash, lsh etc.)\n",
    "deduplicated_output_path = os.path.join(fuzzy_output_dir, \"fuzzy_deduped_dataset\")\n",
    "\n",
    "input_filetype = (\n",
    "    \"parquet\"  # this can be either of jsonl or parquet (you'll need to change how input data is generated)\n",
    ")\n",
    "output_filetype = \"parquet\"  # this can be either of jsonl or parquet\n",
    "\n",
    "storage_options = None  # Optional additional cloud I/O args to pass into Pandas/cuDF during I/O operations.\n",
    "fs, _ = fsspec.url_to_fs(fuzzy_cache_path, **storage_options if storage_options is not None else {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427f692-3e90-4ff8-9f90-a18f071182d6",
   "metadata": {},
   "source": [
    "### Downloading and saving a sample dataset\n",
    "\n",
    "We download and save the [Tinystories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset to the specified `input_dataset_path` above. This step can be skipped if running on a different dataset that's already present in the input_dataset_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f399d1-2486-4b8d-bd40-d3ddbd77dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.utils.file_utils import get_all_file_paths_under\n",
    "\n",
    "if len(get_all_file_paths_under(input_dataset_path, storage_options=storage_options)) == 0:\n",
    "    import os\n",
    "    import uuid\n",
    "\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    input_df = load_dataset(\"roneneldan/TinyStories\", split=\"train\").to_pandas()\n",
    "    num_rows_per_file = 10_000\n",
    "\n",
    "    os.makedirs(input_dataset_path, exist_ok=True)\n",
    "\n",
    "    for i, start_idx in enumerate(range(0, len(input_df), num_rows_per_file)):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing file {i}\")\n",
    "        end_idx = min(len(input_df), start_idx + num_rows_per_file)\n",
    "        subset_df = input_df.iloc[start_idx:end_idx].copy()\n",
    "        subset_df[\"id\"] = [str(uuid.uuid4()) for _ in range(len(subset_df))]\n",
    "        subset_df.to_parquet(\n",
    "            os.path.join(input_dataset_path, f\"part_{i}.parquet\"), index=False, storage_options=storage_options\n",
    "        )\n",
    "\n",
    "    print(f\"Created {i + 1} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabcb72d-6d22-4324-a51c-73241f0d9d19",
   "metadata": {},
   "source": [
    "## Running as a Single Stage (End-to-End)\n",
    "\n",
    "See the [API Reference](https://docs.nvidia.com/nemo/curator/latest/apidocs/stages/stages.deduplication.fuzzy.workflow.html#api) for more information about the `FuzzyDeduplicationWorkflow` class.\n",
    "\n",
    "### General Notes\n",
    "#### ID Generation\n",
    "1. The Fuzzy Deduplication Workflow doesn't utilize any existing IDs in the input dataset and instead generates IDs on the fly using an ID Generator actor.\n",
    "2. The ID Generator gives each row a unique increasing integer ID, based on the order files are read.\n",
    "3. This avoids expensive ID->Integer encoding for the underlying connected components algorithm which only supports integer IDs.\n",
    "4. When we find duplicates, we save these integer IDs in sorted files with multiple row groups.\n",
    "5. We also save a `fuzzy_id_generator.json` which maintains a mapping of input file partitions to ID ranges for that batch.\n",
    "6. During removal, reading the same file groups will give the same integer IDs, using the min/max ID values, we can find all corresponding duplicates in that range making the process faster.\n",
    "\n",
    "#### Performance Considerations\n",
    "1. LSH - Configuring `bands_per_iteration` controls how many bands to process simultaneously in a single shuffle. Higher values can lead to faster performance but might increase memory pressure.\n",
    "2. A low `input_blocksize` may not saturate the GPUs enough while a high `input_blocksize` can lead to OOM errors during MinHash and excessive object store usage during removal. It's recommend to keep it at 1-1.5GiB and reduce if running into OOMs during MinHash.\n",
    "3. The removal step can be memory intensive and it's recommend to set a higher fraction of object store memory for removal (if the machine has enough RAM). The `RayDataExecutor` showed better results during duplicate removal.\n",
    "4. The removal workflow is CPU only and can be run  on machines that don't have GPUs\n",
    "\n",
    "#### Hyperparameter Considerations\n",
    "1. The current defaults for fuzzy deduplication (260 hashes, 13 hashes per band) approximate finding documents with a Jaccard similarity of 0.8. For more information on selecting the number of bands/hashes it's recommended to analyze the S curve and tolerable threshold for false positives (and negatives). More information about LSH can be found in section `3.4.2` [here](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf).\n",
    "2. The `char_ngrams` values of 24 is set to approximate roughly ngrams that correspond to ~5 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf7a7a3-4659-45b3-be86-5803efb54f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.stages.deduplication.fuzzy import FuzzyDeduplicationWorkflow\n",
    "from nemo_curator.stages.deduplication.id_generator import CURATOR_DEDUP_ID_STR\n",
    "from nemo_curator.stages.text.deduplication import TextDuplicatesRemovalWorkflow\n",
    "\n",
    "# All workflows support passing in different kwargs and storage_options for the read, cache and output datasets\n",
    "# We use a common one here for simplicity\n",
    "io_kwargs = {\"storage_options\": storage_options} if storage_options is not None else None\n",
    "\n",
    "identification_workflow = FuzzyDeduplicationWorkflow(\n",
    "    cache_path=fuzzy_cache_path,\n",
    "    output_path=fuzzy_output_dir,\n",
    "    input_path=input_dataset_path,\n",
    "    input_filetype=input_filetype,\n",
    "    input_blocksize=\"1GiB\",\n",
    "    text_field=\"text\",\n",
    "    seed=42,\n",
    "    char_ngrams=24,\n",
    "    minhashes_per_band=13,\n",
    "    bands_per_iteration=10,\n",
    "    read_kwargs=io_kwargs,\n",
    "    cache_kwargs=io_kwargs,\n",
    "    write_kwargs=io_kwargs,\n",
    ")\n",
    "\n",
    "removal_workflow = TextDuplicatesRemovalWorkflow(\n",
    "    input_path=input_dataset_path,  # Must be identical to the path used during identification\n",
    "    ids_to_remove_path=os.path.join(fuzzy_output_dir, \"FuzzyDuplicateIds\"),\n",
    "    output_path=deduplicated_output_path,\n",
    "    input_filetype=input_filetype,\n",
    "    input_blocksize=\"1GiB\",  # This must be identical to the blocksize used during identification\n",
    "    ids_to_remove_duplicate_id_field=CURATOR_DEDUP_ID_STR,\n",
    "    id_generator_path=os.path.join(fuzzy_output_dir, \"fuzzy_id_generator.json\"),\n",
    "    output_filetype=\"parquet\",\n",
    "    input_kwargs=io_kwargs,  # read_kwargs for input dataset\n",
    "    ids_to_remove_read_kwargs=io_kwargs,  # read_kwargs for removal_id's generated by Fuzzy workflow\n",
    "    id_generator_storage_options=storage_options,\n",
    "    output_kwargs=io_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7329ff2-3a36-42ad-833a-9589653d0e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-09 03:10:36.332\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnemo_curator.core.client\u001b[0m:\u001b[36mstart\u001b[0m:\u001b[36m106\u001b[0m - \u001b[33m\u001b[1mNo monitoring services are running. Please run the `start_prometheus_grafana.py` script from nemo_curator/metrics folder to setup monitoring services separately.\u001b[0m\n",
      "\u001b[32m2025-12-09 03:10:36.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.core.utils\u001b[0m:\u001b[36minit_cluster\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1mRay start command: ray start --head --node-ip-address 127.0.1.1 --port 6380 --metrics-export-port 8081 --dashboard-host 127.0.0.1 --dashboard-port 8267 --ray-client-server-port 20000 --temp-dir /tmp/ray --disable-usage-stats --num-gpus 2 --num-cpus 64 --block\u001b[0m\n",
      "2025-12-09 03:10:36,390\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 03:10:36,395\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "[2025-12-09 03:10:44,056 W 87202 87202] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-12-09 03:10:45,058 W 87202 87202] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-12-09 03:10:46,059 W 87202 87202] global_state_accessor.cc:505: Some processes that the driver needs to connect to have not registered with GCS, so retrying. Have you run 'ray start' on this node?\n",
      "[2025-12-09 03:10:47,060 I 87202 87202] global_state_accessor.cc:487: This node has an IP address of 127.0.1.1, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.\n",
      "2025-12-09 03:10:47,063\tINFO worker.py:2012 -- Connected to Ray cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 03:10:39,174\tINFO usage_lib.py:447 -- Usage stats collection is disabled.\n",
      "2025-12-09 03:10:39,175\tINFO scripts.py:914 -- \u001b[37mLocal node IP\u001b[39m: \u001b[1m127.0.1.1\u001b[22m\n",
      "2025-12-09 03:10:47,028\tSUCC scripts.py:950 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-12-09 03:10:47,028\tSUCC scripts.py:951 -- \u001b[32mRay runtime started.\u001b[39m\n",
      "2025-12-09 03:10:47,028\tSUCC scripts.py:952 -- \u001b[32m--------------------\u001b[39m\n",
      "2025-12-09 03:10:47,028\tINFO scripts.py:954 -- \u001b[36mNext steps\u001b[39m\n",
      "2025-12-09 03:10:47,028\tINFO scripts.py:957 -- To add another node to this Ray cluster, run\n",
      "2025-12-09 03:10:47,028\tINFO scripts.py:960 -- \u001b[1m  ray start --address='127.0.1.1:6380'\u001b[22m\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:969 -- To connect to this Ray cluster:\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:971 -- \u001b[35mimport\u001b[39m\u001b[26m ray\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:972 -- ray\u001b[35m.\u001b[39m\u001b[26minit(_node_ip_address\u001b[35m=\u001b[39m\u001b[26m\u001b[33m'127.0.1.1'\u001b[39m\u001b[26m)\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:984 -- To submit a Ray job using the Ray Jobs CLI:\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:985 -- \u001b[1m  RAY_API_SERVER_ADDRESS='http://127.0.0.1:8267' ray job submit --working-dir . -- python my_script.py\u001b[22m\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:994 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:998 -- for more information on submitting Ray jobs to the Ray cluster.\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:1003 -- To terminate the Ray runtime, run\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:1004 -- \u001b[1m  ray stop\u001b[22m\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:1007 -- To view the status of the cluster, use\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:1008 --   \u001b[1mray status\u001b[22m\u001b[26m\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:1012 -- To monitor and debug Ray, view the dashboard at \n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:1013 --   \u001b[1m127.0.0.1:8267\u001b[22m\u001b[26m\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:1020 -- \u001b[4mIf connection to the dashboard fails, check your firewall settings and network configuration.\u001b[24m\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:1121 -- \u001b[36m\u001b[1m--block\u001b[22m\u001b[39m\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:1122 -- This command will now block forever until terminated by a signal.\n",
      "2025-12-09 03:10:47,029\tINFO scripts.py:1125 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[32m2025-12-09 03:10:51.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.pipeline.pipeline\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mPlanning pipeline: minhash_pipeline\u001b[0m\n",
      "2025-12-09 03:10:51,086\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 03:10:51,091\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 03:10:51,097\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[32m2025-12-09 03:10:51.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.utils\u001b[0m:\u001b[36mexecute_setup_on_node\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mExecuting setup on node f0b30e122c7ad6b4e779347d12e6eb559cf781bebc1691f36d7a2e56 for 2 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 03:10:57.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSetup on node complete for all stages. Starting Ray Actor Pool pipeline with 2 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 03:10:57.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 1/2: FilePartitioningStage(file_paths='./input', files_per_partition=None, blocksize='1GiB', file_extensions=['.jsonl', '.json', '.parquet'], storage_options={}, limit=None, name='file_partitioning')\u001b[0m\n",
      "\u001b[32m2025-12-09 03:10:57.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 1\u001b[0m\n",
      "\u001b[32m2025-12-09 03:10:57.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=128, GPU limit=2147483647\u001b[0m\n",
      "\u001b[32m2025-12-09 03:10:57.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:10:57.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 0.5 CPUs, 0.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:10:57.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m FilePartitioningStage(file_paths='./input', files_per_partition=None, blocksize='1GiB', file_extensions=['.jsonl', '.json', '.parquet'], storage_options={}, limit=None, name='file_partitioning') - Creating 1 actors (CPUs: 0.5, GPUs: 0.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 03:10:57.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for file_partitioning with 1 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:01.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m290\u001b[0m - \u001b[1mBroke down 1 tasks into batches of 1 for a total of 1 batches for file_partitioning\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:01.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 1\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:01.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 2/2: MinHashStage\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:01.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 1\u001b[0m\n",
      "\u001b[36m(FilePartitioningStage pid=92879)\u001b[0m 2025-12-09 03:11:01.398 | INFO     | nemo_curator.stages.file_partitioning:process:95 - Found 212 files\n",
      "\u001b[36m(FilePartitioningStage pid=92879)\u001b[0m 2025-12-09 03:11:01.398 | INFO     | nemo_curator.stages.file_partitioning:process:132 - Created 1 file groups from 212 files\n",
      "\u001b[32m2025-12-09 03:11:01.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:01.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:01.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 1.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:01.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m MinHashStage - Creating 1 actors (CPUs: 1.0, GPUs: 1.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:01.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for MinHashStage with 1 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:12.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m290\u001b[0m - \u001b[1mBroke down 1 tasks into batches of 1 for a total of 1 batches for MinHashStage\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:15.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 1\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:15.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1m\n",
      "Pipeline completed. Final results: 1 tasks\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:15.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mShutting down Ray to clean up all resources...\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:16.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mMinhash pipeline completed in 25.23 seconds\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:16.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.pipeline.pipeline\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mPlanning pipeline: lsh_duplicate_identification_pipeline\u001b[0m\n",
      "2025-12-09 03:11:16,321\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 03:11:16,325\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 03:11:16,331\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[32m2025-12-09 03:11:16.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.utils\u001b[0m:\u001b[36mexecute_setup_on_node\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mExecuting setup on node f0b30e122c7ad6b4e779347d12e6eb559cf781bebc1691f36d7a2e56 for 2 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:26.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSetup on node complete for all stages. Starting Ray Actor Pool pipeline with 2 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:26.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 1/2: FilePartitioningStage(file_paths='./fuzzy_outputs/cache/MinHashStage', files_per_partition=None, blocksize='2GiB', file_extensions=['.parquet'], storage_options={}, limit=None, name='file_partitioning')\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:26.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 1\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:26.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=128, GPU limit=2147483647\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:26.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:26.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 0.5 CPUs, 0.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:26.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m FilePartitioningStage(file_paths='./fuzzy_outputs/cache/MinHashStage', files_per_partition=None, blocksize='2GiB', file_extensions=['.parquet'], storage_options={}, limit=None, name='file_partitioning') - Creating 1 actors (CPUs: 0.5, GPUs: 0.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:26.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for file_partitioning with 1 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:30.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m290\u001b[0m - \u001b[1mBroke down 1 tasks into batches of 1 for a total of 1 batches for file_partitioning\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:30.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 1\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:30.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 2/2: LSHStage(num_bands=20, minhashes_per_band=13, id_field='_curator_dedup_id', minhash_field='_minhash_signature', output_path='./fuzzy_outputs/cache', read_kwargs={}, write_kwargs={}, rmm_pool_size='auto', spill_memory_limit='auto', enable_statistics=False, bands_per_iteration=10, total_nparts=None)\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:30.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 1\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:30.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m371\u001b[0m - \u001b[1m  Processing band range: 0-10\u001b[0m\n",
      "\u001b[36m(FilePartitioningStage pid=97511)\u001b[0m 2025-12-09 03:11:30.965 | INFO     | nemo_curator.stages.file_partitioning:process:95 - Found 1 files\n",
      "\u001b[36m(FilePartitioningStage pid=97511)\u001b[0m 2025-12-09 03:11:30.965 | INFO     | nemo_curator.stages.file_partitioning:process:132 - Created 1 file groups from 1 files\n",
      "\u001b[32m2025-12-09 03:11:31.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:31.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:31.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 1.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:31.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m383\u001b[0m - \u001b[1m LSHStage(num_bands=20, minhashes_per_band=13, id_field='_curator_dedup_id', minhash_field='_minhash_signature', output_path='./fuzzy_outputs/cache', read_kwargs={}, write_kwargs={}, rmm_pool_size='auto', spill_memory_limit='auto', enable_statistics=False, bands_per_iteration=10, total_nparts=None) - Creating 1 actors (CPUs: 1.0, GPUs: 1.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:31.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m387\u001b[0m - \u001b[1m  Creating RapidsMPFShuffling Actor Pool for stage: LSHStage\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:31.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1m    Initializing RapidsMPFShuffling actor pool with 1 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:31.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m224\u001b[0m - \u001b[1m    Setting up UCXX communication...\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:42.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1m    UCXX setup complete\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:42.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m371\u001b[0m - \u001b[1m  Processing band range: 10-20\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:43.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:43.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:43.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 1.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:43.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m383\u001b[0m - \u001b[1m LSHStage(num_bands=20, minhashes_per_band=13, id_field='_curator_dedup_id', minhash_field='_minhash_signature', output_path='./fuzzy_outputs/cache', read_kwargs={}, write_kwargs={}, rmm_pool_size='auto', spill_memory_limit='auto', enable_statistics=False, bands_per_iteration=10, total_nparts=None) - Creating 1 actors (CPUs: 1.0, GPUs: 1.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:43.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m387\u001b[0m - \u001b[1m  Creating RapidsMPFShuffling Actor Pool for stage: LSHStage\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:43.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1m    Initializing RapidsMPFShuffling actor pool with 1 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:43.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m224\u001b[0m - \u001b[1m    Setting up UCXX communication...\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:55.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1m    UCXX setup complete\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:56.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_execute_lsh_stage\u001b[0m:\u001b[36m396\u001b[0m - \u001b[1m  LSH processing complete. Output tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:56.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1m\n",
      "Pipeline completed. Final results: 2 tasks\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:56.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mShutting down Ray to clean up all resources...\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:56.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m315\u001b[0m - \u001b[1mLSH pipeline completed in 40.54 seconds\u001b[0m\n",
      "\u001b[32m2025-12-09 03:11:56.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.pipeline.pipeline\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mPlanning pipeline: connected_components_pipeline\u001b[0m\n",
      "2025-12-09 03:11:56,865\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 03:11:56,869\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 03:11:56,875\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[32m2025-12-09 03:11:56.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.utils\u001b[0m:\u001b[36mexecute_setup_on_node\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mExecuting setup on node f0b30e122c7ad6b4e779347d12e6eb559cf781bebc1691f36d7a2e56 for 3 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:04.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSetup on node complete for all stages. Starting Ray Actor Pool pipeline with 3 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:04.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 1/3: BucketsToEdgesStage\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:04.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:04.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2147483647\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:04.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:04.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 0.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:04.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m BucketsToEdgesStage - Creating 2 actors (CPUs: 1.0, GPUs: 0.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:04.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for BucketsToEdgesStage with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:11.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m290\u001b[0m - \u001b[1mBroke down 2 tasks into batches of 1 for a total of 2 batches for BucketsToEdgesStage\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:26.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:26.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 2/3: ConnectedComponentsStage\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:26.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:26.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:26.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:26.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 1.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:26.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m ConnectedComponentsStage - Creating 2 actors (CPUs: 1.0, GPUs: 1.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:26.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1m  Creating RAFT actor pool for stage: ConnectedComponentsStage\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:26.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_raft_actor_pool\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1m    Initializing RAFT actor pool with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:26.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_raft_actor_pool\u001b[0m:\u001b[36m193\u001b[0m - \u001b[1m    Setting up RAFT communication...\u001b[0m\n",
      "\u001b[36m(ConnectedComponentsStage pid=98640)\u001b[0m 2025-12-09 03:12:34.834 | WARNING  | nemo_curator.backends.experimental.ray_actor_pool.raft_adapter:__init__:59 - batch size not set for stage ConnectedComponentsStage. Setting it to 1.\n",
      "\u001b[32m2025-12-09 03:12:40.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_raft_actor_pool\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1m    RAFT setup complete\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:40.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for ConnectedComponentsStage with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:40.297\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m276\u001b[0m - \u001b[33m\u001b[1mStage ConnectedComponentsStage is a RAFT stage but has a batch size of 1. Ignoring batch size.\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:40.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_process_stage_with_pool\u001b[0m:\u001b[36m286\u001b[0m - \u001b[1mDistributed 2 tasks evenly across 2 actors for RAFT stage ConnectedComponentsStage\u001b[0m\n",
      "\u001b[36m(ConnectedComponentsStage pid=98641)\u001b[0m 2025-12-09 03:12:46.472 | INFO     | nemo_curator.stages.deduplication.fuzzy.connected_components:weakly_connected_components:152 - Computing weakly connected components completed successfully!\n",
      "\u001b[36m(ConnectedComponentsStage pid=98641)\u001b[0m 2025-12-09 03:12:34.981 | WARNING  | nemo_curator.backends.experimental.ray_actor_pool.raft_adapter:__init__:59 - batch size not set for stage ConnectedComponentsStage. Setting it to 1.\n",
      "\u001b[32m2025-12-09 03:12:46.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:46.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\n",
      "Processing stage 3/3: IdentifyDuplicatesStage\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:46.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Input tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:46.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m    Resource calculation: CPU limit=64, GPU limit=2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:46.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m    Available: 64.0 CPUs, 2.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:46.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.utils\u001b[0m:\u001b[36mcalculate_optimal_actors_for_stage\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1m    Stage requirements: 1.0 CPUs, 1.0 GPUs\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:46.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1m IdentifyDuplicatesStage - Creating 2 actors (CPUs: 1.0, GPUs: 1.0)\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:46.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1m  Creating Shuffle actors for stage: IdentifyDuplicates\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:46.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1m    Initializing RapidsMPFShuffling actor pool with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:46.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m224\u001b[0m - \u001b[1m    Setting up UCXX communication...\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:56.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36m_create_rapidsmpf_actors\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1m    UCXX setup complete\u001b[0m\n",
      "\u001b[32m2025-12-09 03:12:56.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mCreated actor pool for IdentifyDuplicates with 2 actors\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:00.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m  Output tasks: 2\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:00.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1m\n",
      "Pipeline completed. Final results: 2 tasks\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:00.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_actor_pool.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mShutting down Ray to clean up all resources...\u001b[0m\n",
      "\u001b[36m(ConnectedComponentsStage pid=98640)\u001b[0m 2025-12-09 03:12:46.469 | INFO     | nemo_curator.stages.deduplication.fuzzy.connected_components:weakly_connected_components:152 - Computing weakly connected components completed successfully!\n",
      "\u001b[32m2025-12-09 03:13:00.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1mConnected components pipeline completed in 63.56 seconds\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:00.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m333\u001b[0m - \u001b[1mNumber of documents removed: 320471\u001b[0m\n",
      "2025-12-09 03:13:00,427\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 03:13:00,431\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 03:13:00,437\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[32m2025-12-09 03:13:00.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mId generator written to ./fuzzy_outputs/fuzzy_id_generator.json\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:00.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.stages.deduplication.fuzzy.workflow\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m347\u001b[0m - \u001b[1mFuzzy deduplication pipeline completed in 129.38 seconds\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:00.465\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[33m\u001b[1mRay Data executor is experimental and might not work as expected.\u001b[0m\n",
      "2025-12-09 03:13:00,472\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 03:13:00,476\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 03:13:00,476\tINFO worker.py:1850 -- Calling ray.init() again after it has already been called.\n",
      "\u001b[32m2025-12-09 03:13:01.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.pipeline.pipeline\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mPlanning pipeline: text_duplicates_removal_workflow\u001b[0m\n",
      "2025-12-09 03:13:01,051\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 03:13:01,055\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 03:13:01,063\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[32m2025-12-09 03:13:05.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.utils\u001b[0m:\u001b[36mexecute_setup_on_node\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mExecuting setup on node f0b30e122c7ad6b4e779347d12e6eb559cf781bebc1691f36d7a2e56 for 4 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mSetup on node complete for all stages. Starting Ray Data pipeline with 4 stages\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mProcessing stage 1/4: FilePartitioningStage(file_paths='./input', files_per_partition=None, blocksize='1GiB', file_extensions=['.jsonl', '.json', '.parquet'], storage_options={}, limit=None, name='file_partitioning')\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1m  CPU cores: 0.5, GPU ratio: 0.0\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.adapter\u001b[0m:\u001b[36mprocess_dataset\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mFilePartitioningStage is_actor_stage_=False with concurrency_kwargs={'concurrency': None, 'num_cpus': 0.5}\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mProcessing stage 2/4: ParquetReaderStage(fields=None, read_kwargs=None, name='parquet_reader', _generate_ids=False, _assign_ids=True)\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1m  CPU cores: 1.0, GPU ratio: 0.0\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.adapter\u001b[0m:\u001b[36mprocess_dataset\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mParquetReaderStage is_actor_stage_=True with concurrency_kwargs={'concurrency': (1, 64), 'num_cpus': 1.0}\u001b[0m\n",
      "2025-12-09 03:13:09,457\tWARNING util.py:597 -- The argument ``concurrency`` is deprecated in Ray 2.51. Please specify argument ``compute`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "\u001b[32m2025-12-09 03:13:09.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mProcessing stage 3/4: TextDuplicatesRemovalStage(ids_to_remove_path='./fuzzy_outputs/FuzzyDuplicateIds', id_field='_curator_dedup_id', duplicate_id_field='_curator_dedup_id', read_kwargs={})\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1m  CPU cores: 1.0, GPU ratio: 0.0\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.adapter\u001b[0m:\u001b[36mprocess_dataset\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mTextDuplicatesRemovalStage is_actor_stage_=False with concurrency_kwargs={'concurrency': None, 'num_cpus': 1.0}\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mProcessing stage 4/4: ParquetWriter(path='./fuzzy_outputs/fuzzy_deduped_dataset', file_extension='parquet', write_kwargs={}, fields=None, name='parquet_writer', mode='ignore', append_mode_implemented=False)\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1m  CPU cores: 1.0, GPU ratio: 0.0\u001b[0m\n",
      "\u001b[32m2025-12-09 03:13:09.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.adapter\u001b[0m:\u001b[36mprocess_dataset\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mParquetWriter is_actor_stage_=False with concurrency_kwargs={'concurrency': None, 'num_cpus': 1.0}\u001b[0m\n",
      "2025-12-09 03:13:09,494\tINFO streaming_executor.py:85 -- A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True`.\n",
      "2025-12-09 03:13:09,495\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_5_0\n",
      "2025-12-09 03:13:09,507\tINFO streaming_executor.py:170 -- Starting execution of Dataset dataset_5_0. Full logs are in /tmp/ray/session_2025-12-09_03-10-39_176553_87351/logs/ray-data\n",
      "2025-12-09 03:13:09,507\tINFO streaming_executor.py:171 -- Execution plan of Dataset dataset_5_0: InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(FilePartitioningStageTask)] -> TaskPoolMapOperator[StreamingRepartition] -> ActorPoolMapOperator[MapBatches(ParquetReaderStageActor)] -> TaskPoolMapOperator[MapBatches(TextDuplicatesRemovalStageTask)->MapBatches(ParquetWriterTask)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ab11e45b9f439bbfe8edcfc71e3bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bc6b899d34405bb1b5dc68d60a9836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(FilePartitioningStageTask) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f8a00b00314359b012f3c2e6e8dd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- StreamingRepartition 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c9ea13312241928e9b1bcc0e3bd8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(ParquetReaderStageActor) 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5088f46494b744878f2e1f13c17eb837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(TextDuplicatesRemovalStageTask)->MapBatches(ParquetWriterTask) 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 03:13:09,576\tWARNING resource_manager.py:134 --   Ray's object store is configured to use only 10.2% of available memory (186.3GiB out of 1826.1GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "\u001b[36m(MapBatches(FilePartitioningStageTask) pid=99439)\u001b[0m 2025-12-09 03:13:09.821 | INFO     | nemo_curator.stages.file_partitioning:process:95 - Found 212 files\n",
      "\u001b[36m(MapBatches(FilePartitioningStageTask) pid=99439)\u001b[0m 2025-12-09 03:13:09.822 | INFO     | nemo_curator.stages.file_partitioning:process:132 - Created 1 file groups from 212 files\n",
      "2025-12-09 03:13:39,195\tINFO streaming_executor.py:298 --   Dataset dataset_5_0 execution finished in 29.69 seconds\n",
      "2025-12-09 03:13:39,203\tINFO util.py:257 -- Exiting prefetcher's background thread\n",
      "\u001b[32m2025-12-09 03:13:39.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.backends.experimental.ray_data.executor\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mPipeline completed. Final results: 1 tasks\u001b[0m\n",
      "2025-12-09 03:13:39,296\tINFO worker.py:1691 -- Using address 127.0.1.1:6380 set in the environment variable RAY_ADDRESS\n",
      "2025-12-09 03:13:39,300\tINFO worker.py:1832 -- Connecting to existing Ray cluster at address: 127.0.1.1:6380...\n",
      "2025-12-09 03:13:39,306\tINFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[32m2025-12-09 03:13:42.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.core.client\u001b[0m:\u001b[36mstop\u001b[0m:\u001b[36m181\u001b[0m - \u001b[1mNeMo Curator has stopped the Ray cluster it started by killing the Ray GCS process. It is advised to wait for a few seconds before running any Ray commands to ensure Ray can cleanup other processes.If you are seeing any Ray commands like `ray status` failing, please ensure /tmp/ray/ray_current_cluster has correct information.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.backends.experimental.ray_data import RayDataExecutor\n",
    "from nemo_curator.core.client import RayClient\n",
    "\n",
    "client = RayClient(num_cpus=64, num_gpus=2)  # change as needed\n",
    "client.start()\n",
    "\n",
    "_ = identification_workflow.run()\n",
    "_ = removal_workflow.run(executor=RayDataExecutor())\n",
    "\n",
    "client.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c983f3-f562-45e6-bd09-5660cb179aa3",
   "metadata": {},
   "source": [
    "### Looking at Intermediate Results and Output\n",
    "\n",
    "#### MinHash Results\n",
    "1. `_curator_dedup_id` - The IDs assigned to this dataset on the fly during the initial read.\n",
    "2. `_minhash_signature` - MinHash Signature\n",
    "\n",
    "#### LSH Results\n",
    "1. `_bucket_id` - The bucket/band identifier\n",
    "2. `_curator_dedup_id` - List of all document IDs that belong to that bucket\n",
    "\n",
    "#### Buckets To Edges Result\n",
    "1. `_curator_dedup_id_x`, `_curator_dedup_id_y` - Mapping of edges in a Graph where each column are documents that are potential duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d9b66f-0d48-4a90-96d6-570fd218777c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_minhash_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[11644717, 429172, 6014805, 86354, 2387151, 49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[2103321, 653305, 2941429, 5780991, 6977799, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[1891498, 3797631, 2961751, 50078, 21382505, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[1286357, 4060996, 1376561, 3044837, 7369355, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[6272013, 12535265, 819579, 5975720, 25677928,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id                                 _minhash_signature\n",
       "0                  0  [11644717, 429172, 6014805, 86354, 2387151, 49...\n",
       "1                  1  [2103321, 653305, 2941429, 5780991, 6977799, 7...\n",
       "2                  2  [1891498, 3797631, 2961751, 50078, 21382505, 5...\n",
       "3                  3  [1286357, 4060996, 1376561, 3044837, 7369355, ...\n",
       "4                  4  [6272013, 12535265, 819579, 5975720, 25677928,..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_bucket_id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0_000055fd7daae1e46223e8b7e06bf2e0</td>\n",
       "      <td>[1178375, 2079489]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0_00006a5f30f7b2c96588bfc1bfb5321a</td>\n",
       "      <td>[365218, 1933514]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0_00006f316d5bd251bd83702e3f1e017f</td>\n",
       "      <td>[161590, 771961]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b0_0000d90e9e4140a7ac31e6b227a62f62</td>\n",
       "      <td>[8290, 567169]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b0_0000f975e5bcda25838df43b0d37737f</td>\n",
       "      <td>[965853, 1334885]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _bucket_id   _curator_dedup_id\n",
       "0  b0_000055fd7daae1e46223e8b7e06bf2e0  [1178375, 2079489]\n",
       "1  b0_00006a5f30f7b2c96588bfc1bfb5321a   [365218, 1933514]\n",
       "2  b0_00006f316d5bd251bd83702e3f1e017f    [161590, 771961]\n",
       "3  b0_0000d90e9e4140a7ac31e6b227a62f62      [8290, 567169]\n",
       "4  b0_0000f975e5bcda25838df43b0d37737f   [965853, 1334885]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id_x</th>\n",
       "      <th>_curator_dedup_id_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1178375</td>\n",
       "      <td>2079489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>365218</td>\n",
       "      <td>1933514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161590</td>\n",
       "      <td>771961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8290</td>\n",
       "      <td>567169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>965853</td>\n",
       "      <td>1334885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id_x  _curator_dedup_id_y\n",
       "0              1178375              2079489\n",
       "1               365218              1933514\n",
       "2               161590               771961\n",
       "3                 8290               567169\n",
       "4               965853              1334885"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minhash_path = os.path.join(fuzzy_cache_path, \"MinHashStage\")\n",
    "display(pd.read_parquet(fs.unstrip_protocol(fs.find(minhash_path)[0]), storage_options=storage_options).head())\n",
    "\n",
    "lsh_path = os.path.join(fuzzy_cache_path, \"LSHStage\")\n",
    "display(pd.read_parquet(fs.unstrip_protocol(fs.find(lsh_path)[0]), storage_options=storage_options).head())\n",
    "\n",
    "b2e_path = os.path.join(fuzzy_cache_path, \"BucketsToEdgesStage\")\n",
    "display(pd.read_parquet(fs.unstrip_protocol(fs.find(b2e_path)[0]), storage_options=storage_options).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650db74e-b764-44fa-966a-e4f0ddcb7182",
   "metadata": {},
   "source": [
    "#### Connected Components Result\n",
    "\n",
    "1. `_curator_dedup_id` - The document IDs\n",
    "2. `_duplicate_group_id` - The group ID that document belongs to. Documents with the same duplicate group ID are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2549cfe9-3003-414c-a5cb-666e458b4615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>577</td>\n",
       "      <td>482274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>579</td>\n",
       "      <td>161180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>581</td>\n",
       "      <td>161181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640509</th>\n",
       "      <td>2119669</td>\n",
       "      <td>640509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640510</th>\n",
       "      <td>2119670</td>\n",
       "      <td>480105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640511</th>\n",
       "      <td>2119671</td>\n",
       "      <td>480106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640512</th>\n",
       "      <td>2119673</td>\n",
       "      <td>480107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640513</th>\n",
       "      <td>2119675</td>\n",
       "      <td>640513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640514 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        _curator_dedup_id  _duplicate_group_id\n",
       "0                     576                    0\n",
       "1                     577               482274\n",
       "2                     578                    2\n",
       "3                     579               161180\n",
       "4                     581               161181\n",
       "...                   ...                  ...\n",
       "640509            2119669               640509\n",
       "640510            2119670               480105\n",
       "640511            2119671               480106\n",
       "640512            2119673               480107\n",
       "640513            2119675               640513\n",
       "\n",
       "[640514 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "_duplicate_group_id\n",
       "0              [576, 187440]\n",
       "2              [578, 187442]\n",
       "6              [584, 187448]\n",
       "8              [586, 187450]\n",
       "11             [591, 187455]\n",
       "                 ...        \n",
       "640505    [1942710, 2119661]\n",
       "640506    [1942713, 2119664]\n",
       "640507    [1942715, 2119666]\n",
       "640509    [1942718, 2119669]\n",
       "640513    [1942724, 2119675]\n",
       "Name: _curator_dedup_id, Length: 320043, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "_duplicate_group_id\n",
       "476285    230\n",
       "534130      3\n",
       "78584       3\n",
       "88204       3\n",
       "558429      3\n",
       "         ... \n",
       "106810      2\n",
       "623214      2\n",
       "106808      2\n",
       "623212      2\n",
       "636876      2\n",
       "Name: count, Length: 320043, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc_path = os.path.join(fuzzy_cache_path, \"ConnectedComponentsStage\")\n",
    "cc_df = pd.read_parquet(cc_path, storage_options=storage_options)  # works with pandas since the input here is small\n",
    "display(cc_df)\n",
    "grouped_cc_df = cc_df.groupby(\"_duplicate_group_id\")._curator_dedup_id.agg(list)\n",
    "display(grouped_cc_df)\n",
    "duplicate_cluster_sizes = cc_df._duplicate_group_id.value_counts()\n",
    "display(duplicate_cluster_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87d677-40aa-49e1-bf6d-ab41a0941957",
   "metadata": {},
   "source": [
    "Based on the distribution above we can see that there is one cluster/group where 230 documents are all duplicates followed by many smaller clusters with 2/3 documents that are duplicates.\n",
    "\n",
    "#### FuzzyDuplicateIds Results (List of duplicate docs to remove)\n",
    "1. `_curator_dedup_id` - ID of docs in the removal list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda82d7a-8b35-4e64-8af0-5163bf1be875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _curator_dedup_id\n",
       "0                577\n",
       "1                591\n",
       "2                593\n",
       "3                597\n",
       "4                598"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate documents found for removal: 320471\n"
     ]
    }
   ],
   "source": [
    "duplicate_ids_path = os.path.join(fuzzy_output_dir, \"FuzzyDuplicateIds\")\n",
    "duplicates_df = pd.read_parquet(duplicate_ids_path, storage_options=storage_options)\n",
    "display(duplicates_df.head())\n",
    "\n",
    "print(f\"Number of duplicate documents found for removal: {len(duplicates_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af14853-2900-4d41-8792-6ff2b1cafb4c",
   "metadata": {},
   "source": [
    "#### Checking that the duplicate ids list contains only one document per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a1f8bf4-8e1b-425c-91f1-3d312bf17786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the duplicate group: 230\n",
      "Number of documents in the removal list from the same group: 229\n"
     ]
    }
   ],
   "source": [
    "# As an example let's look at the group with the largest number of duplicates\n",
    "largest_duplicate_cluster = grouped_cc_df.loc[duplicate_cluster_sizes.index[0]]\n",
    "\n",
    "# number of docs in the removal list from this group\n",
    "docs_to_remove_in_group = duplicates_df._curator_dedup_id.isin(largest_duplicate_cluster).sum()\n",
    "\n",
    "print(f\"Number of documents in the duplicate group: {len(largest_duplicate_cluster)}\")\n",
    "print(f\"Number of documents in the removal list from the same group: {docs_to_remove_in_group}\")\n",
    "assert docs_to_remove_in_group == (len(largest_duplicate_cluster) - 1)  # noqa: S101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a6829-3566-4558-8b18-1fd5d0191d40",
   "metadata": {},
   "source": [
    "#### Advanced: Looking at examples of duplicate documents\n",
    "\n",
    "1. This analysis involves re-reading the input data with the same ID mapping that was used during duplicate identification.\n",
    "2. Merging the input data with the connected components results on the `_curator_dedup_id` column to associate each document which the duplicate group it belongs to which can be used for further analysis.\n",
    "\n",
    "**NOTE**: This analysis approach is intended as an example for smaller datasets and only works for cases where the connected components dataframe is small and fits comfortable in memory. It is not recommended for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef243e5-b8df-47d5-b993-fa61a3954eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.pipeline import Pipeline\n",
    "from nemo_curator.stages.base import ProcessingStage\n",
    "from nemo_curator.stages.resources import Resources\n",
    "from nemo_curator.stages.text.io.reader import ParquetReader\n",
    "from nemo_curator.tasks.document import DocumentBatch\n",
    "\n",
    "\n",
    "class CustomMergeStage(ProcessingStage[DocumentBatch, DocumentBatch]):\n",
    "    \"\"\"\n",
    "    Warning: This should not be attempted with large connected components results.\n",
    "    A small stage that merges the input data (using the id's generated) with the connected components result.\n",
    "    Works because CC results are small enough to fit per batch.\n",
    "    \"\"\"\n",
    "\n",
    "    resources = Resources(cpus=1.0)\n",
    "\n",
    "    def process(self, batch: DocumentBatch) -> DocumentBatch:\n",
    "        df = batch.to_pandas().merge(cc_df, how=\"inner\", on=[CURATOR_DEDUP_ID_STR])\n",
    "        return DocumentBatch(\n",
    "            task_id=batch.task_id, dataset_name=batch.dataset_name, data=df, _stage_perf=batch._stage_perf\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"Explore duplicates\",\n",
    "    stages=[\n",
    "        ParquetReader(file_paths=input_dataset_path, blocksize=\"1GiB\", _assign_ids=True, read_kwargs=io_kwargs),\n",
    "        CustomMergeStage(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44018e57-6748-4c39-aaf6-5caa2d1df247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.core.client import RayClient\n",
    "from nemo_curator.stages.deduplication.id_generator import create_id_generator_actor, kill_id_generator_actor\n",
    "\n",
    "os.environ[\"RAY_ADDRESS\"] = \"\"  # reset the ray address from the previous GPU cluster\n",
    "client = RayClient(num_cpus=8)  # change as needed\n",
    "client.start()\n",
    "\n",
    "create_id_generator_actor(\n",
    "    filepath=os.path.join(fuzzy_output_dir, \"fuzzy_id_generator.json\"), storage_options=storage_options\n",
    ")\n",
    "merged_results = pipeline.run()\n",
    "merged_df = pd.concat([batch.to_pandas() for batch in merged_results]).sort_values(\"_duplicate_group_id\")\n",
    "kill_id_generator_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "070d42e8-28c2-4abd-a40f-a0fe103befbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442448</th>\n",
       "      <td></td>\n",
       "      <td>c5b4b736-7c1d-4811-b1fe-1245e408ef55</td>\n",
       "      <td>1470911</td>\n",
       "      <td>476285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300453</th>\n",
       "      <td></td>\n",
       "      <td>9a6a840f-9d2c-4691-b95c-c3eee0183011</td>\n",
       "      <td>985385</td>\n",
       "      <td>476285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192037</th>\n",
       "      <td></td>\n",
       "      <td>13caa084-5173-4708-bffa-63504097a2bb</td>\n",
       "      <td>630289</td>\n",
       "      <td>476285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442447</th>\n",
       "      <td></td>\n",
       "      <td>e11a03ae-eacf-4f2b-bbb8-326ec98225c9</td>\n",
       "      <td>1470910</td>\n",
       "      <td>476285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192049</th>\n",
       "      <td></td>\n",
       "      <td>d6d8106b-d5f8-4392-b4b7-06a8042fb4af</td>\n",
       "      <td>630301</td>\n",
       "      <td>476285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515976</th>\n",
       "      <td></td>\n",
       "      <td>aff0af8a-b08b-4bc5-b520-63ba12803182</td>\n",
       "      <td>1720912</td>\n",
       "      <td>476285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373157</th>\n",
       "      <td></td>\n",
       "      <td>0a593871-cb80-4cf1-bafb-72df442adf5a</td>\n",
       "      <td>1225085</td>\n",
       "      <td>476285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330555</th>\n",
       "      <td></td>\n",
       "      <td>00748418-8f47-4115-9a6c-2193528bbaf9</td>\n",
       "      <td>1083885</td>\n",
       "      <td>476285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118054</th>\n",
       "      <td></td>\n",
       "      <td>1edd5a53-dac5-425f-af45-618bba15d8b6</td>\n",
       "      <td>397188</td>\n",
       "      <td>476285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118055</th>\n",
       "      <td></td>\n",
       "      <td>0a6e3ba7-460f-4ce7-8575-3054afb8e848</td>\n",
       "      <td>397189</td>\n",
       "      <td>476285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text                                    id  _curator_dedup_id  \\\n",
       "442448       c5b4b736-7c1d-4811-b1fe-1245e408ef55            1470911   \n",
       "300453       9a6a840f-9d2c-4691-b95c-c3eee0183011             985385   \n",
       "192037       13caa084-5173-4708-bffa-63504097a2bb             630289   \n",
       "442447       e11a03ae-eacf-4f2b-bbb8-326ec98225c9            1470910   \n",
       "192049       d6d8106b-d5f8-4392-b4b7-06a8042fb4af             630301   \n",
       "...     ...                                   ...                ...   \n",
       "515976       aff0af8a-b08b-4bc5-b520-63ba12803182            1720912   \n",
       "373157       0a593871-cb80-4cf1-bafb-72df442adf5a            1225085   \n",
       "330555       00748418-8f47-4115-9a6c-2193528bbaf9            1083885   \n",
       "118054       1edd5a53-dac5-425f-af45-618bba15d8b6             397188   \n",
       "118055       0a6e3ba7-460f-4ce7-8575-3054afb8e848             397189   \n",
       "\n",
       "        _duplicate_group_id  \n",
       "442448               476285  \n",
       "300453               476285  \n",
       "192037               476285  \n",
       "442447               476285  \n",
       "192049               476285  \n",
       "...                     ...  \n",
       "515976               476285  \n",
       "373157               476285  \n",
       "330555               476285  \n",
       "118054               476285  \n",
       "118055               476285  \n",
       "\n",
       "[230 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(merged_df[merged_df._curator_dedup_id.isin(largest_duplicate_cluster)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc423318-c1d5-4769-ba42-362f515c3166",
   "metadata": {},
   "source": [
    "The largest cluster/group of duplicates in this dataset seems to be all documents with empty/no text.\n",
    "\n",
    "Let's look at the second largest cluster of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a08dc31e-d4f9-47e0-b8b7-7f75e4e6abef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>_curator_dedup_id</th>\n",
       "      <th>_duplicate_group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227663</th>\n",
       "      <td>Sara and Ben are friends. They like to play to...</td>\n",
       "      <td>44f5c27d-ea91-48c0-946d-e4eaca60114e</td>\n",
       "      <td>745466</td>\n",
       "      <td>534130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182026</th>\n",
       "      <td>Sara and Ben are friends. They like to play to...</td>\n",
       "      <td>479c928d-4033-47d6-9abe-d2ac9df008a1</td>\n",
       "      <td>600335</td>\n",
       "      <td>534130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371194</th>\n",
       "      <td>Sara and Ben are friends. They like to play to...</td>\n",
       "      <td>a8ec3d5a-8693-4985-8254-71b6fbc0393f</td>\n",
       "      <td>1218218</td>\n",
       "      <td>534130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "227663  Sara and Ben are friends. They like to play to...   \n",
       "182026  Sara and Ben are friends. They like to play to...   \n",
       "371194  Sara and Ben are friends. They like to play to...   \n",
       "\n",
       "                                          id  _curator_dedup_id  \\\n",
       "227663  44f5c27d-ea91-48c0-946d-e4eaca60114e             745466   \n",
       "182026  479c928d-4033-47d6-9abe-d2ac9df008a1             600335   \n",
       "371194  a8ec3d5a-8693-4985-8254-71b6fbc0393f            1218218   \n",
       "\n",
       "        _duplicate_group_id  \n",
       "227663               534130  \n",
       "182026               534130  \n",
       "371194               534130  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document1\n",
      "----------\n",
      "Sara and Ben are friends. They like to play together. One day, they go to the park with their moms. There are many things to play with in the park. There are swings, slides, seesaws and sand.\n",
      "\n",
      "Sara and Ben run to the swings. They take turns to push each other. They go high and low, high and low. They laugh and shout. \"Wee!\" Sara says. \"This is fun!\" Ben says.\n",
      "\n",
      "Then they see a big box near the slide. It is open. They are curious. They go to the box and look inside. There are many toys in the box. There are balls, dolls, cars, books and puzzles. They are jolly. They clap their hands. \"Wow!\" Sara says. \"Look at all these toys!\" Ben says.\n",
      "\n",
      "They take out some toys and play with them. They roll the balls, dress the dolls, drive the cars, read the books and make the puzzles. They share and help each other. They are happy. They smile and hug.\n",
      "\n",
      "Their moms come to the box and see them playing. They are proud. They smile and hug too. \"You are good friends,\" Sara's mom says. \"You are kind and polite,\" Ben's mom says.\n",
      "\n",
      "Sara and Ben nod. They are good friends. They are kind and polite. They are jolly. They love to play.\n",
      "\n",
      "Document2\n",
      "----------\n",
      "Sara and Ben are friends. They like to play together. One day, they go to the park with their moms. There are many things to play with in the park. There are swings, slides, seesaws and sand.\n",
      "\n",
      "Sara and Ben run to the swings. They take turns to push each other. They go high and low, high and low. They laugh and shout. \"Wee!\" Sara says. \"This is fun!\" Ben says.\n",
      "\n",
      "Then they see a big box near the slide. It is open. They are curious. They go to the box and look inside. There are many toys in the box. There are balls, dolls, cars, books and puzzles. They are jolly. They clap their hands. \"Wow!\" Sara says. \"Look at all these toys!\" Ben says.\n",
      "\n",
      "They take out some toys and play with them. They roll the balls, dress the dolls, drive the cars, read the books and make the puzzles. They share and help each other. They are happy. They smile and hug.\n",
      "\n",
      "Their moms come to the box and see them playing. They are proud. They smile and hug too. \"You are good friends,\" Sara's mom says. \"You are kind and polite,\" Ben's mom says.\n",
      "\n",
      "Sara and Ben nod. They are good friends. They are kind and polite. They are jolly. They love to play.\n"
     ]
    }
   ],
   "source": [
    "duplicates = merged_df[merged_df._curator_dedup_id.isin(grouped_cc_df.loc[duplicate_cluster_sizes.index[1]])]\n",
    "display(duplicates)\n",
    "\n",
    "print(f\"\\nDocument1\\n----------\\n{duplicates.iloc[0].text}\")\n",
    "print(f\"\\nDocument2\\n----------\\n{duplicates.iloc[1].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be9aca1d-f26a-4767-a9bc-9019969da01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-09 03:16:50.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnemo_curator.core.client\u001b[0m:\u001b[36mstop\u001b[0m:\u001b[36m181\u001b[0m - \u001b[1mNeMo Curator has stopped the Ray cluster it started by killing the Ray GCS process. It is advised to wait for a few seconds before running any Ray commands to ensure Ray can cleanup other processes.If you are seeing any Ray commands like `ray status` failing, please ensure /tmp/ray/ray_current_cluster has correct information.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "client.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016ea8c-26b0-451b-b436-09ebdef9a5df",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We were able to find and remove ~320_000 duplicate documents in a dataset of ~2.1 Million Rows "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
